{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyPTffTLug7i"
      },
      "source": [
        "# **Laboratorio 11: LLM y Agentes Autónomos 🤖**\n",
        "\n",
        "MDS7202: Laboratorio de Programación Científica para Ciencia de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pbWVyntzbvL"
      },
      "source": [
        "### **Cuerpo Docente:**\n",
        "\n",
        "- Profesores: Ignacio Meza, Sebastián Tinoco\n",
        "- Auxiliar: Eduardo Moya\n",
        "- Ayudantes: Nicolás Ojeda, Melanie Peña, Valentina Rojas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy6ikgVYzghB"
      },
      "source": [
        "### Equipo: Ratas.py 🐁\n",
        "\n",
        "- Nombre de alumno 1: Geraldyn Pérez\n",
        "- Nombre de alumno 2: Diego Rojas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMJ-owchzjFf"
      },
      "source": [
        "### **Link de repositorio de GitHub:** [Repositorio](https://github.com/Geral37/MDS7202.git)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUuwsXrKzmkK"
      },
      "source": [
        "## **Temas a tratar**\n",
        "\n",
        "- Reinforcement Learning\n",
        "- Large Language Models\n",
        "\n",
        "## **Reglas:**\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n",
        "- Prohibidas las copias.\n",
        "- Pueden usar cualquer matrial del curso que estimen conveniente.\n",
        "\n",
        "### **Objetivos principales del laboratorio**\n",
        "\n",
        "- Resolución de problemas secuenciales usando Reinforcement Learning\n",
        "- Habilitar un Chatbot para entregar respuestas útiles usando Large Language Models.\n",
        "\n",
        "El laboratorio deberá ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al máximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante más eficientes que los iteradores nativos sobre DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hmHHQ9BuyAG"
      },
      "source": [
        "## **1. Reinforcement Learning (2.0 puntos)**\n",
        "\n",
        "En esta sección van a usar métodos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gOcejYb6uzOO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22c1c921-9895-4d37-a888-8e3e2acbc8fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/958.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qqq gymnasium stable_baselines3\n",
        "!pip install -qqq swig\n",
        "!pip install -qqq gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBPet_Mq8dX9"
      },
      "source": [
        "### **1.1 Blackjack (1.0 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "La idea de esta subsección es que puedan implementar métodos de RL y así generar una estrategia para jugar el clásico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
        "\n",
        "Comencemos primero preparando el ambiente. El siguiente bloque de código transforma las observaciones del ambiente a `np.array`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LpZ8bBKk9ZlU"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.spaces import MultiDiscrete\n",
        "import numpy as np\n",
        "\n",
        "class FlattenObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(FlattenObservation, self).__init__(env)\n",
        "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.array(observation).flatten()\n",
        "\n",
        "# Create and wrap the environment\n",
        "env = gym.make(\"Blackjack-v1\")\n",
        "env = FlattenObservation(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ6J1_-Y9nHO"
      },
      "source": [
        "#### **1.1.1 Descripción de MDP (0.2 puntos)**\n",
        "\n",
        "Entregue una breve descripción sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulación en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5i1Wt1p770x"
      },
      "source": [
        "`escriba su respuesta acá`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmcX6bRC9agQ"
      },
      "source": [
        "#### **1.1.2 Generando un Baseline (0.2 puntos)**\n",
        "\n",
        "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulación 5000 veces y reporte el promedio y desviación de las recompensas. ¿Cómo calificaría el performance de esta política? ¿Cómo podría interpretar las recompensas obtenidas?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9p2PrLLR9yju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3db57c8-ab90-41f4-bad8-c71e3ee6afa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promedio de recompensas: -0.4002\n",
            "Desviación estándar de las recompensas: 0.893554676558743\n"
          ]
        }
      ],
      "source": [
        "total_rewards = [] # recompensas\n",
        "\n",
        "# Repetir la simulación 5000 veces\n",
        "for _ in range(5000):\n",
        "    # Reiniciar el entorno\n",
        "    observation, _ = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        # Elegir una acción aleatoria (el espacio de acción tiene 2 dimensiones: \"acción 0\" o \"acción 1\")\n",
        "        action = env.action_space.sample()  # Esto selecciona una acción aleatoria\n",
        "\n",
        "        # Tomar la acción en el entorno\n",
        "        observation, reward, done, _, _ = env.step(action)\n",
        "\n",
        "        # Acumular la recompensa\n",
        "        total_reward += reward\n",
        "\n",
        "    # Guardar la recompensa total del episodio\n",
        "    total_rewards.append(total_reward)\n",
        "\n",
        "# Convertir la lista de recompensas a un array de numpy para análisis\n",
        "total_rewards = np.array(total_rewards)\n",
        "\n",
        "# Calcular el promedio y desviación estándar\n",
        "average_reward = np.mean(total_rewards)\n",
        "std_deviation = np.std(total_rewards)\n",
        "\n",
        "# Reportar los resultados\n",
        "print(f\"Promedio de recompensas: {average_reward}\")\n",
        "print(f\"Desviación estándar de las recompensas: {std_deviation}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es mala perfomance porque la estrategia siempre entrega pérdida en valor esperado. El juego aleatorio estaría cargado en contra del jugador."
      ],
      "metadata": {
        "id": "bEF8x4dLZVeR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEO_dY4x_SJu"
      },
      "source": [
        "#### **1.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
        "\n",
        "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "m9JsFA1wGmnH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1a5308d-cc01-4d56-f859-4fa811fac3c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 1.09     |\n",
            "|    ep_rew_mean        | -0.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 137      |\n",
            "|    iterations         | 100      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 500      |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.247   |\n",
            "|    explained_variance | 0.536    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 99       |\n",
            "|    policy_loss        | -0.0453  |\n",
            "|    value_loss         | 0.566    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 1.04     |\n",
            "|    ep_rew_mean        | -0.27    |\n",
            "| time/                 |          |\n",
            "|    fps                | 199      |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 1000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.196   |\n",
            "|    explained_variance | 0.028    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | -0.00398 |\n",
            "|    value_loss         | 0.623    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 1.02     |\n",
            "|    ep_rew_mean        | -0.11    |\n",
            "| time/                 |          |\n",
            "|    fps                | 233      |\n",
            "|    iterations         | 300      |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 1500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0667  |\n",
            "|    explained_variance | -0.428   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 299      |\n",
            "|    policy_loss        | 0.00204  |\n",
            "|    value_loss         | 1.37     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 1        |\n",
            "|    ep_rew_mean        | -0.15    |\n",
            "| time/                 |          |\n",
            "|    fps                | 256      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0797  |\n",
            "|    explained_variance | 0.642    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | -0.00682 |\n",
            "|    value_loss         | 0.348    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 1        |\n",
            "|    ep_rew_mean        | -0.25    |\n",
            "| time/                 |          |\n",
            "|    fps                | 272      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0283  |\n",
            "|    explained_variance | nan      |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | -0.00192 |\n",
            "|    value_loss         | 0.182    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 1        |\n",
            "|    ep_rew_mean        | -0.14    |\n",
            "| time/                 |          |\n",
            "|    fps                | 284      |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 3000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0153  |\n",
            "|    explained_variance | 0.262    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | 0.000665 |\n",
            "|    value_loss         | 0.905    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 1        |\n",
            "|    ep_rew_mean        | -0.14    |\n",
            "| time/                 |          |\n",
            "|    fps                | 292      |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 3500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0341  |\n",
            "|    explained_variance | 0.101    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | -0.00171 |\n",
            "|    value_loss         | 0.605    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 1        |\n",
            "|    ep_rew_mean        | 0.04     |\n",
            "| time/                 |          |\n",
            "|    fps                | 293      |\n",
            "|    iterations         | 800      |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 4000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0166  |\n",
            "|    explained_variance | -0.951   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 799      |\n",
            "|    policy_loss        | 0.00211  |\n",
            "|    value_loss         | 0.904    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 1        |\n",
            "|    ep_rew_mean        | -0.26    |\n",
            "| time/                 |          |\n",
            "|    fps                | 291      |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 15       |\n",
            "|    total_timesteps    | 4500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.00742 |\n",
            "|    explained_variance | 0.0915   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | 0.000552 |\n",
            "|    value_loss         | 0.99     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 1         |\n",
            "|    ep_rew_mean        | -0.11     |\n",
            "| time/                 |           |\n",
            "|    fps                | 297       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.00578  |\n",
            "|    explained_variance | -0.235    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | -0.000306 |\n",
            "|    value_loss         | 0.846     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 1.01     |\n",
            "|    ep_rew_mean        | -0.31    |\n",
            "| time/                 |          |\n",
            "|    fps                | 302      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 18       |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.00234 |\n",
            "|    explained_variance | -0.0471  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | 0.000161 |\n",
            "|    value_loss         | 1.1      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 1        |\n",
            "|    ep_rew_mean        | -0.35    |\n",
            "| time/                 |          |\n",
            "|    fps                | 307      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 19       |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.00234 |\n",
            "|    explained_variance | 0.154    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | 0.000207 |\n",
            "|    value_loss         | 1.58     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 1        |\n",
            "|    ep_rew_mean        | -0.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 310      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 20       |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.00102 |\n",
            "|    explained_variance | 0.249    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | 3.05e-06 |\n",
            "|    value_loss         | 0.721    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 1         |\n",
            "|    ep_rew_mean        | -0.23     |\n",
            "| time/                 |           |\n",
            "|    fps                | 313       |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.00181  |\n",
            "|    explained_variance | 0.247     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | -4.66e-05 |\n",
            "|    value_loss         | 0.525     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 1         |\n",
            "|    ep_rew_mean        | -0.19     |\n",
            "| time/                 |           |\n",
            "|    fps                | 316       |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.00166  |\n",
            "|    explained_variance | 0.79      |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -9.24e-05 |\n",
            "|    value_loss         | 0.249     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 1        |\n",
            "|    ep_rew_mean        | -0.11    |\n",
            "| time/                 |          |\n",
            "|    fps                | 317      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 25       |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.00213 |\n",
            "|    explained_variance | 0.172    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | 6.23e-05 |\n",
            "|    value_loss         | 0.663    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 1         |\n",
            "|    ep_rew_mean        | -0.19     |\n",
            "| time/                 |           |\n",
            "|    fps                | 314       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.00247  |\n",
            "|    explained_variance | 0.759     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -0.000137 |\n",
            "|    value_loss         | 0.278     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 1         |\n",
            "|    ep_rew_mean        | -0.25     |\n",
            "| time/                 |           |\n",
            "|    fps                | 315       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.00197  |\n",
            "|    explained_variance | 0.598     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | -5.14e-05 |\n",
            "|    value_loss         | 0.269     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 1        |\n",
            "|    ep_rew_mean        | -0.15    |\n",
            "| time/                 |          |\n",
            "|    fps                | 316      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 29       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.00421 |\n",
            "|    explained_variance | 0.238    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | 0.000347 |\n",
            "|    value_loss         | 0.516    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 1        |\n",
            "|    ep_rew_mean        | -0.26    |\n",
            "| time/                 |          |\n",
            "|    fps                | 318      |\n",
            "|    iterations         | 2000     |\n",
            "|    time_elapsed       | 31       |\n",
            "|    total_timesteps    | 10000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.00575 |\n",
            "|    explained_variance | 0.0152   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1999     |\n",
            "|    policy_loss        | -0.00024 |\n",
            "|    value_loss         | 0.835    |\n",
            "------------------------------------\n",
            "Episodio 1 - Recompensa Total: 1.0\n",
            "Episodio 2 - Recompensa Total: 1.0\n",
            "Episodio 3 - Recompensa Total: 1.0\n",
            "Episodio 4 - Recompensa Total: -1.0\n",
            "Episodio 5 - Recompensa Total: -1.0\n",
            "Episodio 6 - Recompensa Total: -1.0\n",
            "Episodio 7 - Recompensa Total: 1.0\n",
            "Episodio 8 - Recompensa Total: -1.0\n",
            "Episodio 9 - Recompensa Total: -1.0\n",
            "Episodio 10 - Recompensa Total: 1.0\n"
          ]
        }
      ],
      "source": [
        "from stable_baselines3 import A2C\n",
        "\n",
        "# Crear el modelo A2C con una red neuronal de 64 neuronas en una capa oculta\n",
        "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
        "\n",
        "# Entrenar el modelo (10000 pasos de entrenamiento)\n",
        "model.learn(total_timesteps=10000)\n",
        "\n",
        "# Evaluar el modelo en 10 episodios\n",
        "for episode in range(10):\n",
        "    obs, info = env.reset()  # Reiniciar el entorno\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        # Elegir una acción usando el modelo entrenado\n",
        "        action, _states = model.predict(obs)\n",
        "\n",
        "        # Ejecutar la acción en el entorno\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "        # Acumular la recompensa\n",
        "        total_reward += reward\n",
        "\n",
        "    print(f\"Episodio {episode + 1} - Recompensa Total: {total_reward}\")\n",
        "\n",
        "# Guardar el modelo entrenado (opcional)\n",
        "model.save(\"a2c_blackjack\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-bpdb8wZID1"
      },
      "source": [
        "#### **1.1.4 Evaluación de modelo (0.2 puntos)**\n",
        "\n",
        "Repita el ejercicio 1.1.2 pero utilizando el modelo entrenado. ¿Cómo es el performance de su agente? ¿Es mejor o peor que el escenario baseline?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5-d7d8GFf7F6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9aaa38a-644b-4ff7-f3e1-b66381857a49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promedio de recompensas: -0.1886\n",
            "Desviación estándar de las recompensas: 0.9582432050372182\n"
          ]
        }
      ],
      "source": [
        "# Cargar el modelo A2C entrenado (si lo tienes guardado)\n",
        "#model = A2C.load(\"a2c_blackjack\")\n",
        "\n",
        "# Lista para almacenar las recompensas\n",
        "total_rewards = []\n",
        "\n",
        "# Repetir la simulación 5000 veces\n",
        "for _ in range(5000):\n",
        "    # Reiniciar el entorno\n",
        "    observation, _ = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        # Elegir una acción usando el modelo entrenado\n",
        "        action, _states = model.predict(observation)\n",
        "\n",
        "        # Tomar la acción en el entorno\n",
        "        observation, reward, done, truncated, _ = env.step(action)\n",
        "\n",
        "        # Acumular la recompensa\n",
        "        total_reward += reward\n",
        "\n",
        "    # Guardar la recompensa total del episodio\n",
        "    total_rewards.append(total_reward)\n",
        "\n",
        "# Convertir la lista de recompensas a un array de numpy para análisis\n",
        "total_rewards = np.array(total_rewards)\n",
        "\n",
        "# Calcular el promedio y desviación estándar\n",
        "average_reward = np.mean(total_rewards)\n",
        "std_deviation = np.std(total_rewards)\n",
        "\n",
        "# Reportar los resultados\n",
        "print(f\"Promedio de recompensas: {average_reward}\")\n",
        "print(f\"Desviación estándar de las recompensas: {std_deviation}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mantiene baja performance pero mejora respecto al baseline. Se obtiene un promedio de pérdidas menor."
      ],
      "metadata": {
        "id": "Iqg4ygJkbkHY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO-EsAaPAYEm"
      },
      "source": [
        "#### **1.1.5 Estudio de acciones (0.2 puntos)**\n",
        "\n",
        "Genere una función que reciba un estado y retorne la accion del agente. Luego, use esta función para entregar la acción escogida frente a los siguientes escenarios:\n",
        "\n",
        "- Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
        "- Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
        "\n",
        "¿Son coherentes sus acciones con las reglas del juego?\n",
        "\n",
        "Hint: ¿A que clase de python pertenecen los estados? Pruebe a usar el método `.reset` para saberlo."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BnvgONQcTC7",
        "outputId": "f65bfdbf-5eb9-47c0-b34d-a81f29c95ec8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([20,  2,  0]), {})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Fh8XlGyzwtRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1ccf1fc-b858-4f73-df68-620cd41c7ae6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acción en el escenario 1 (Agente suma 6, Dealer muestra 7, sin as): 0\n",
            "Acción en el escenario 2 (Agente suma 19, Dealer muestra 3, con as): 0\n"
          ]
        }
      ],
      "source": [
        "# Función para obtener la acción del agente dado un estado\n",
        "def obtener_accion(state):\n",
        "    # Usamos el modelo entrenado para predecir la acción en base al estado\n",
        "    action, _states = model.predict(state)\n",
        "    return action\n",
        "\n",
        "# Primer escenario: Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene un as\n",
        "# Para simular este escenario, creamos un estado artificialmente. Suponemos que el estado es un vector de numpy.\n",
        "# Nota: La representación del estado dependerá de cómo esté estructurado el entorno. Para fines de este ejercicio,\n",
        "# asumimos que el estado es un array que contiene [suma_agente, suma_dealer, tiene_as].\n",
        "# Este es un ejemplo simplificado.\n",
        "\n",
        "estado_escenario_1 = np.array([6, 7, 0])  # Suma del agente es 6, dealer muestra 7, agente no tiene un as\n",
        "accion_1 = obtener_accion(estado_escenario_1)\n",
        "\n",
        "# Segundo escenario: Suma de cartas del agente es 19, dealer muestra un 3, agente tiene un as\n",
        "estado_escenario_2 = np.array([19, 3, 1])  # Suma del agente es 19, dealer muestra 3, agente tiene un as\n",
        "accion_2 = obtener_accion(estado_escenario_2)\n",
        "\n",
        "# Imprimir las acciones en ambos escenarios\n",
        "print(f\"Acción en el escenario 1 (Agente suma 6, Dealer muestra 7, sin as): {accion_1}\")\n",
        "print(f\"Acción en el escenario 2 (Agente suma 19, Dealer muestra 3, con as): {accion_2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEqCTqqroh03"
      },
      "source": [
        "### **1.2 LunarLander**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.redd.it/097t6tk29zf51.jpg\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Similar a la sección 2.1, en esta sección usted se encargará de implementar una gente de RL que pueda resolver el ambiente `LunarLander`.\n",
        "\n",
        "Comencemos preparando el ambiente:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "nvQUyuZ_FtZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3712962-fcee-4987-b9df-7c1f0926e39a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\", continuous = True) # notar el parámetro continuous = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBU4lGX3wpN6"
      },
      "source": [
        "Noten que se especifica el parámetro `continuous = True`. ¿Que implicancias tiene esto sobre el ambiente?\n",
        "\n",
        "Además, se le facilita la función `export_gif` para el ejercicio 2.2.4:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bRiWpSo9yfr9"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "def export_gif(model, n = 5):\n",
        "  '''\n",
        "  función que exporta a gif el comportamiento del agente en n episodios\n",
        "  '''\n",
        "  images = []\n",
        "  for episode in range(n):\n",
        "    obs = model.env.reset()\n",
        "    img = model.env.render()\n",
        "    done = False\n",
        "    while not done:\n",
        "      images.append(img)\n",
        "      action, _ = model.predict(obs)\n",
        "      obs, reward, done, info = model.env.step(action)\n",
        "      img = model.env.render(mode=\"rgb_array\")\n",
        "\n",
        "  imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk5VJVppXh3N"
      },
      "source": [
        "#### **1.2.1 Descripción de MDP (0.2 puntos)**\n",
        "\n",
        "Entregue una breve descripción sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulación en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas. ¿Como se distinguen las acciones de este ambiente en comparación a `Blackjack`?\n",
        "\n",
        "Nota: recuerde que se especificó el parámetro `continuous = True`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb-u9LUE8O9a"
      },
      "source": [
        "`escriba su respuesta acá`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YChodtNQwzG2"
      },
      "source": [
        "#### **1.2.2 Generando un Baseline (0.2 puntos)**\n",
        "\n",
        "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulación 10 veces y reporte el promedio y desviación de las recompensas. ¿Cómo calificaría el performance de esta política?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "5bwc3A0GX7a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e7b8db-d68e-4f5d-d31f-e15d4b25d041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promedio de recompensas: -176.59248950091904\n",
            "Desviación estándar de las recompensas: 113.9127838304885\n"
          ]
        }
      ],
      "source": [
        "# Lista para almacenar las recompensas totales de cada episodio\n",
        "rewards = []\n",
        "\n",
        "# Ejecutar la simulación 10 veces con acciones aleatorias\n",
        "for episode in range(10):\n",
        "    obs, info = env.reset()  # Reiniciar el entorno\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        # Seleccionar una acción aleatoria\n",
        "        action = env.action_space.sample()\n",
        "\n",
        "        # Tomar la acción en el entorno\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "        # Acumular la recompensa total\n",
        "        total_reward += reward\n",
        "\n",
        "    # Guardar la recompensa total del episodio\n",
        "    rewards.append(total_reward)\n",
        "\n",
        "# Convertir la lista de recompensas a un array de numpy para análisis\n",
        "rewards = np.array(rewards)\n",
        "\n",
        "# Calcular el promedio y desviación estándar de las recompensas\n",
        "average_reward = np.mean(rewards)\n",
        "std_deviation = np.std(rewards)\n",
        "\n",
        "# Reportar los resultados\n",
        "print(f\"Promedio de recompensas: {average_reward}\")\n",
        "print(f\"Desviación estándar de las recompensas: {std_deviation}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al igual que el baseline del punto anterior, en promedio se obtiene pérdidas al seguir esta estrategia."
      ],
      "metadata": {
        "id": "eDENs-lRc4l7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQrZVQflX_5f"
      },
      "source": [
        "#### **1.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
        "\n",
        "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "y_6Ia9uoF7Hs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9c8de4c-9a85-46a6-a81f-840c55c7f36a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 74.8     |\n",
            "|    ep_rew_mean     | -428     |\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 222      |\n",
            "|    time_elapsed    | 1        |\n",
            "|    total_timesteps | 299      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 8.1      |\n",
            "|    critic_loss     | 601      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 198      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 111      |\n",
            "|    ep_rew_mean     | -528     |\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 153      |\n",
            "|    time_elapsed    | 5        |\n",
            "|    total_timesteps | 890      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.12     |\n",
            "|    critic_loss     | 251      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 789      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 125      |\n",
            "|    ep_rew_mean     | -436     |\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 158      |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 1495     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.53     |\n",
            "|    critic_loss     | 33       |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 1394     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 117      |\n",
            "|    ep_rew_mean     | -390     |\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 160      |\n",
            "|    time_elapsed    | 11       |\n",
            "|    total_timesteps | 1870     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 11.6     |\n",
            "|    critic_loss     | 107      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 1769     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 117      |\n",
            "|    ep_rew_mean     | -384     |\n",
            "| time/              |          |\n",
            "|    episodes        | 20       |\n",
            "|    fps             | 161      |\n",
            "|    time_elapsed    | 14       |\n",
            "|    total_timesteps | 2346     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 11.7     |\n",
            "|    critic_loss     | 50.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 2245     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 117      |\n",
            "|    ep_rew_mean     | -363     |\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 155      |\n",
            "|    time_elapsed    | 18       |\n",
            "|    total_timesteps | 2816     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 13.3     |\n",
            "|    critic_loss     | 49.9     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 2715     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 119      |\n",
            "|    ep_rew_mean     | -345     |\n",
            "| time/              |          |\n",
            "|    episodes        | 28       |\n",
            "|    fps             | 157      |\n",
            "|    time_elapsed    | 21       |\n",
            "|    total_timesteps | 3323     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 14.4     |\n",
            "|    critic_loss     | 63.4     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 3222     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 160      |\n",
            "|    ep_rew_mean     | -331     |\n",
            "| time/              |          |\n",
            "|    episodes        | 32       |\n",
            "|    fps             | 145      |\n",
            "|    time_elapsed    | 35       |\n",
            "|    total_timesteps | 5136     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 11.9     |\n",
            "|    critic_loss     | 25.1     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 5035     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 173      |\n",
            "|    ep_rew_mean     | -309     |\n",
            "| time/              |          |\n",
            "|    episodes        | 36       |\n",
            "|    fps             | 144      |\n",
            "|    time_elapsed    | 43       |\n",
            "|    total_timesteps | 6229     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 11.3     |\n",
            "|    critic_loss     | 9.37     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 6128     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 186      |\n",
            "|    ep_rew_mean     | -298     |\n",
            "| time/              |          |\n",
            "|    episodes        | 40       |\n",
            "|    fps             | 146      |\n",
            "|    time_elapsed    | 50       |\n",
            "|    total_timesteps | 7432     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 10.8     |\n",
            "|    critic_loss     | 14.1     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 7331     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 197      |\n",
            "|    ep_rew_mean     | -286     |\n",
            "| time/              |          |\n",
            "|    episodes        | 44       |\n",
            "|    fps             | 144      |\n",
            "|    time_elapsed    | 59       |\n",
            "|    total_timesteps | 8656     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 10.6     |\n",
            "|    critic_loss     | 9.72     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8555     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 203      |\n",
            "|    ep_rew_mean     | -276     |\n",
            "| time/              |          |\n",
            "|    episodes        | 48       |\n",
            "|    fps             | 144      |\n",
            "|    time_elapsed    | 67       |\n",
            "|    total_timesteps | 9752     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 15.6     |\n",
            "|    critic_loss     | 39.8     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 9651     |\n",
            "---------------------------------\n",
            "Episodio 1 - Recompensa Total: -0.5125905198993053\n",
            "Episodio 2 - Recompensa Total: -236.6246804248129\n",
            "Episodio 3 - Recompensa Total: -95.19419173746005\n",
            "Episodio 4 - Recompensa Total: -87.28171059528711\n",
            "Episodio 5 - Recompensa Total: -472.9718423042766\n",
            "Episodio 6 - Recompensa Total: -218.9579092574579\n",
            "Episodio 7 - Recompensa Total: -92.20795229994594\n",
            "Episodio 8 - Recompensa Total: -102.21142433159403\n",
            "Episodio 9 - Recompensa Total: -138.20396393606546\n",
            "Episodio 10 - Recompensa Total: -111.18706775407091\n"
          ]
        }
      ],
      "source": [
        "from stable_baselines3 import TD3\n",
        "\n",
        "# Crear el modelo A2C con una red neuronal de 64 neuronas en una capa oculta\n",
        "model2 = TD3(\"MlpPolicy\", env, verbose=1)\n",
        "\n",
        "# Entrenar el modelo (10000 pasos de entrenamiento)\n",
        "model2.learn(total_timesteps=10000)\n",
        "\n",
        "# Evaluar el modelo en 10 episodios\n",
        "for episode in range(10):\n",
        "    obs, info = env.reset()  # Reiniciar el entorno\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        # Elegir una acción usando el modelo entrenado\n",
        "        action, _states = model2.predict(obs)\n",
        "\n",
        "        # Ejecutar la acción en el entorno\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "        # Acumular la recompensa\n",
        "        total_reward += reward\n",
        "\n",
        "    print(f\"Episodio {episode + 1} - Recompensa Total: {total_reward}\")\n",
        "\n",
        "# Guardar el modelo entrenado (opcional)\n",
        "model2.save(\"td2_LunarLander\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z-oIUSrlAsY"
      },
      "source": [
        "#### **1.2.4 Evaluación de modelo (0.2 puntos)**\n",
        "\n",
        "Repita el ejercicio 1.2.2 pero utilizando el modelo entrenado. ¿Cómo es el performance de su agente? ¿Es mejor o peor que el escenario baseline?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ophyU3KrWrwl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "102e3043-b596-4da8-9a61-74cca1acb7e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promedio de recompensas: -190.66586398909587\n",
            "Desviación estándar de las recompensas: 128.5650290282243\n"
          ]
        }
      ],
      "source": [
        "# Cargar el modelo A2C entrenado (si lo tienes guardado)\n",
        "#model2 = TD3.load(\"td2_LunarLander\")\n",
        "\n",
        "# Lista para almacenar las recompensas\n",
        "total_rewards = []\n",
        "\n",
        "# Repetir la simulación 5000 veces\n",
        "for _ in range(5000):\n",
        "    # Reiniciar el entorno\n",
        "    observation, _ = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        # Elegir una acción usando el modelo entrenado\n",
        "        action, _states = model2.predict(observation)\n",
        "\n",
        "        # Tomar la acción en el entorno\n",
        "        observation, reward, done, truncated, _ = env.step(action)\n",
        "\n",
        "        # Acumular la recompensa\n",
        "        total_reward += reward\n",
        "\n",
        "    # Guardar la recompensa total del episodio\n",
        "    total_rewards.append(total_reward)\n",
        "\n",
        "# Convertir la lista de recompensas a un array de numpy para análisis\n",
        "total_rewards = np.array(total_rewards)\n",
        "\n",
        "# Calcular el promedio y desviación estándar\n",
        "average_reward = np.mean(total_rewards)\n",
        "std_deviation = np.std(total_rewards)\n",
        "\n",
        "# Reportar los resultados\n",
        "print(f\"Promedio de recompensas: {average_reward}\")\n",
        "print(f\"Desviación estándar de las recompensas: {std_deviation}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6Xw4YHT3P5d"
      },
      "source": [
        "#### **1.2.5 Optimización de modelo (0.2 puntos)**\n",
        "\n",
        "Repita los ejercicios 1.2.3 y 1.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente parámetros como:\n",
        "- `total_timesteps`\n",
        "- `learning_rate`\n",
        "- `batch_size`\n",
        "\n",
        "Una vez optimizado el modelo, use la función `export_gif` para estudiar el comportamiento de su agente en la resolución del ambiente y comente sobre sus resultados.\n",
        "\n",
        "Adjunte el gif generado en su entrega (mejor aún si además adjuntan el gif en el markdown)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aItYF6sr6F_6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPUY-Ktgf2BO"
      },
      "source": [
        "## **2. Large Language Models (4.0 puntos)**\n",
        "\n",
        "En esta sección se enfocarán en habilitar un Chatbot que nos permita responder preguntas útiles a través de LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ4fPRRihGLe"
      },
      "source": [
        "### **2.0 Configuración Inicial**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/uqAs9atZH58AAAAd/config-config-issue.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Como siempre, cargamos todas nuestras API KEY al entorno:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud2Xm_k-hFJn"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
        "\n",
        "if \"TAVILY_API_KEY\" not in os.environ:\n",
        "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj9JvQUsgZZJ"
      },
      "source": [
        "### **2.1 Retrieval Augmented Generation (1.5 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://y.yarn.co/218aaa02-c47e-4ec9-b1c9-07792a06a88f_text.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "El objetivo de esta subsección es que habiliten un chatbot que pueda responder preguntas usando información contenida en documentos PDF a través de **Retrieval Augmented Generation.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrxOQroVnaZ5"
      },
      "source": [
        "#### **2.1.1 Reunir Documentos (0 puntos)**\n",
        "\n",
        "Reuna documentos PDF sobre los que hacer preguntas siguiendo las siguientes instrucciones:\n",
        "  - 2 documentos .pdf como mínimo.\n",
        "  - 50 páginas de contenido como mínimo entre todos los documentos.\n",
        "  - Ideas para documentos: Documentos relacionados a temas académicos, laborales o de ocio. Aprovechen este ejercicio para construir algo útil y/o relevante para ustedes!\n",
        "  - Deben ocupar documentos reales, no pueden utilizar los mismos de la clase.\n",
        "  - Deben registrar sus documentos en la siguiente [planilla](https://docs.google.com/spreadsheets/d/1Hy1w_dOiG2UCHJ8muyxhdKPZEPrrL7BNHm6E90imIIM/edit?usp=sharing). **NO PUEDEN USAR LOS MISMOS DOCUMENTOS QUE OTRO GRUPO**\n",
        "  - **Recuerden adjuntar los documentos en su entrega**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D1tIRCi4oJJ"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzq2TjWCnu15"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "\n",
        "doc_paths = [] # rellenar con los path a sus documentos\n",
        "\n",
        "assert len(doc_paths) >= 2, \"Deben adjuntar un mínimo de 2 documentos\"\n",
        "\n",
        "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in doc_paths)\n",
        "assert total_paginas >= 50, f\"Páginas insuficientes: {total_paginas}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r811-P71nizA"
      },
      "source": [
        "#### **2.1.2 Vectorizar Documentos (0.2 puntos)**\n",
        "\n",
        "Vectorice los documentos y almacene sus representaciones de manera acorde."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-yXAdCSn4JM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAUkP5zrnyBK"
      },
      "source": [
        "#### **2.1.3 Habilitar RAG (0.3 puntos)**\n",
        "\n",
        "Habilite la solución RAG a través de una *chain* y guárdela en una variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPIySdDFn99l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycg5S5i_n-kL"
      },
      "source": [
        "#### **2.1.4 Verificación de respuestas (0.5 puntos)**\n",
        "\n",
        "Genere un listado de 3 tuplas (\"pregunta\", \"respuesta correcta\") y analice la respuesta de su solución para cada una. ¿Su solución RAG entrega las respuestas que esperaba?\n",
        "\n",
        "Ejemplo de tupla:\n",
        "- Pregunta: ¿Quién es el presidente de Chile?\n",
        "- Respuesta correcta: El presidente de Chile es Gabriel Boric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_UiEn1hoZYR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8d5zTMHoUgF"
      },
      "source": [
        "#### **2.1.5 Sensibilidad de Hiperparámetros (0.5 puntos)**\n",
        "\n",
        "Extienda el análisis del punto 2.1.4 analizando cómo cambian las respuestas entregadas cambiando los siguientes hiperparámetros:\n",
        "- `Tamaño del chunk`. (*¿Cómo repercute que los chunks sean mas grandes o chicos?*)\n",
        "- `La cantidad de chunks recuperados`. (*¿Qué pasa si se devuelven muchos/pocos chunks?*)\n",
        "- `El tipo de búsqueda`. (*¿Cómo afecta el tipo de búsqueda a las respuestas de mi RAG?*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDh_QgeXLGHc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENJiPPM0giX8"
      },
      "source": [
        "### **2.2 Agentes (1.0 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/rcqnN2aJCSEAAAAd/secret-agent-man.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Similar a la sección anterior, en esta sección se busca habilitar **Agentes** para obtener información a través de tools y así responder la pregunta del usuario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V47l7Mjfrk0N"
      },
      "source": [
        "#### **2.2.1 Tool de Tavily (0.2 puntos)**\n",
        "\n",
        "Generar una *tool* que pueda hacer consultas al motor de búsqueda **Tavily**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6SLKwcWr0AG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SonB1A-9rtRq"
      },
      "source": [
        "#### **2.2.2 Tool de Wikipedia (0.2 puntos)**\n",
        "\n",
        "Generar una *tool* que pueda hacer consultas a **Wikipedia**.\n",
        "\n",
        "*Hint: Le puede ser de ayuda el siguiente [link](https://python.langchain.com/v0.1/docs/modules/tools/).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehJJpoqsr26-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvUIMdX6r0ne"
      },
      "source": [
        "#### **2.2.3 Crear Agente (0.3 puntos)**\n",
        "\n",
        "Crear un agente que pueda responder preguntas preguntas usando las *tools* antes generadas. Asegúrese que su agente responda en español. Por último, guarde el agente en una variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD1_n0wrsDI5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKV0JxK3r-XG"
      },
      "source": [
        "#### **2.2.4 Verificación de respuestas (0.3 puntos)**\n",
        "\n",
        "Pruebe el funcionamiento de su agente y asegúrese que el agente esté ocupando correctamente las tools disponibles. ¿En qué casos el agente debería ocupar la tool de Tavily? ¿En qué casos debería ocupar la tool de Wikipedia?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqo2dsxvywW_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZbDTYiogquv"
      },
      "source": [
        "### **2.3 Multi Agente (1.5 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/r7QMJLxU4BoAAAAd/this-is-getting-out-of-hand-star-wars.gif\"\n",
        "\" width=\"450\">\n",
        "</p>\n",
        "\n",
        "El objetivo de esta subsección es encapsular las funcionalidades creadas en una solución multiagente con un **supervisor**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-iUfH0WvI6m"
      },
      "source": [
        "#### **2.3.1 Generando Tools (0.5 puntos)**\n",
        "\n",
        "Transforme la solución RAG de la sección 2.1 y el agente de la sección 2.2 a *tools* (una tool por cada uno)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw1cfTtvv1AZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQYNjT_0vPCg"
      },
      "source": [
        "#### **2.3.2 Agente Supervisor (0.5 puntos)**\n",
        "\n",
        "Habilite un agente que tenga acceso a las tools del punto anterior y pueda responder preguntas relacionadas. Almacene este agente en una variable llamada supervisor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv2ZY0BAv1RD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea3zWlvyvY7K"
      },
      "source": [
        "#### **2.3.3 Verificación de respuestas (0.25 puntos)**\n",
        "\n",
        "Pruebe el funcionamiento de su agente repitiendo las preguntas realizadas en las secciones 2.1.4 y 2.2.4 y comente sus resultados. ¿Cómo varían las respuestas bajo este enfoque?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_1t0zkgv1qW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb8bdAmYvgwn"
      },
      "source": [
        "#### **2.3.4 Análisis (0.25 puntos)**\n",
        "\n",
        "¿Qué diferencias tiene este enfoque con la solución *Router* vista en clases? Nombre al menos una ventaja y desventaja."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAUlJxqoLK5r"
      },
      "source": [
        "`escriba su respuesta acá`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JWVSuWiZ8Mj"
      },
      "source": [
        "### **2.4 Memoria (Bonus +0.5 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/Gs95aiElrscAAAAd/memory-unlocked-ratatouille-critic.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Una de las principales falencias de las soluciones que hemos visto hasta ahora es que nuestro chat no responde las interacciones anteriores, por ejemplo:\n",
        "\n",
        "- Pregunta 1: \"Hola! mi nombre es Sebastián\"\n",
        "  - Respuesta esperada: \"Hola Sebastián! ...\"\n",
        "- Pregunta 2: \"Cual es mi nombre?\"\n",
        "  - Respuesta actual: \"Lo siento pero no conozco tu nombre :(\"\n",
        "  - **Respuesta esperada: \"Tu nombre es Sebastián\"**\n",
        "\n",
        "Para solucionar esto, se les solicita agregar un componente de **memoria** a la solución entregada en el punto 2.3.\n",
        "\n",
        "**Nota: El Bonus es válido <u>sólo para la sección 2 de Large Language Models.</u>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6Y7tIPJLPfB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFc3jBT5g0kT"
      },
      "source": [
        "### **2.5 Despliegue (0 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/IytHqOp52EsAAAAd/you-get-a-deploy-deploy.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Una vez tengan los puntos anteriores finalizados, toca la etapa de dar a conocer lo que hicimos! Para eso, vamos a desplegar nuestro modelo a través de `gradio`, una librería especializada en el levantamiento rápido de demos basadas en ML.\n",
        "\n",
        "Primero instalamos la librería:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8TsvnCPbkIA"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJBztEUovKsF"
      },
      "source": [
        "Luego sólo deben ejecutar el siguiente código e interactuar con la interfaz a través del notebook o del link generado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3KedQSvg1-n"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "def agent_response(message, history):\n",
        "  '''\n",
        "  Función para gradio, recibe mensaje e historial, devuelte la respuesta del chatbot.\n",
        "  '''\n",
        "  # get chatbot response\n",
        "  response = ... # rellenar con la respuesta de su chat\n",
        "\n",
        "  # assert\n",
        "  assert type(response) == str, \"output de route_question debe ser string\"\n",
        "\n",
        "  # \"streaming\" response\n",
        "  for i in range(len(response)):\n",
        "    time.sleep(0.015)\n",
        "    yield response[: i+1]\n",
        "\n",
        "gr.ChatInterface(\n",
        "    agent_response,\n",
        "    type=\"messages\",\n",
        "    title=\"Chatbot MDS7202\", # Pueden cambiar esto si lo desean\n",
        "    description=\"Hola! Soy un chatbot muy útil :)\", # también la descripción\n",
        "    theme=\"soft\",\n",
        "    ).launch(\n",
        "        share=True, # pueden compartir el link a sus amig@s para que interactuen con su chat!\n",
        "        debug = False,\n",
        "        )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}