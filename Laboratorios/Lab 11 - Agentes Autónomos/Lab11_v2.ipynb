{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyPTffTLug7i"
   },
   "source": [
    "# **Laboratorio 11: LLM y Agentes Autónomos 🤖**\n",
    "\n",
    "MDS7202: Laboratorio de Programación Científica para Ciencia de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pbWVyntzbvL"
   },
   "source": [
    "### **Cuerpo Docente:**\n",
    "\n",
    "- Profesores: Ignacio Meza, Sebastián Tinoco\n",
    "- Auxiliar: Eduardo Moya\n",
    "- Ayudantes: Nicolás Ojeda, Melanie Peña, Valentina Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy6ikgVYzghB"
   },
   "source": [
    "### Equipo: Ratas.py 🐁\n",
    "\n",
    "- Nombre de alumno 1: Geraldyn Pérez\n",
    "- Nombre de alumno 2: Diego Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMJ-owchzjFf"
   },
   "source": [
    "### **Link de repositorio de GitHub:** [Repositorio](https://github.com/Geral37/MDS7202.git)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUuwsXrKzmkK"
   },
   "source": [
    "## **Temas a tratar**\n",
    "\n",
    "- Reinforcement Learning\n",
    "- Large Language Models\n",
    "\n",
    "## **Reglas:**\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n",
    "- Prohibidas las copias.\n",
    "- Pueden usar cualquer matrial del curso que estimen conveniente.\n",
    "\n",
    "### **Objetivos principales del laboratorio**\n",
    "\n",
    "- Resolución de problemas secuenciales usando Reinforcement Learning\n",
    "- Habilitar un Chatbot para entregar respuestas útiles usando Large Language Models.\n",
    "\n",
    "El laboratorio deberá ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al máximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante más eficientes que los iteradores nativos sobre DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hmHHQ9BuyAG"
   },
   "source": [
    "## **1. Reinforcement Learning (2.0 puntos)**\n",
    "\n",
    "En esta sección van a usar métodos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gOcejYb6uzOO",
    "outputId": "7c977d9e-6aa9-4117-c11c-d992477100ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/958.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m952.3/958.1 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -qqq gymnasium stable_baselines3\n",
    "!pip install -qqq swig\n",
    "!pip install -qqq gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBPet_Mq8dX9"
   },
   "source": [
    "### **1.1 Blackjack (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "La idea de esta subsección es que puedan implementar métodos de RL y así generar una estrategia para jugar el clásico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
    "\n",
    "Comencemos primero preparando el ambiente. El siguiente bloque de código transforma las observaciones del ambiente a `np.array`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpZ8bBKk9ZlU"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import MultiDiscrete\n",
    "import numpy as np\n",
    "\n",
    "class FlattenObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(FlattenObservation, self).__init__(env)\n",
    "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.array(observation).flatten()\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "env = FlattenObservation(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ6J1_-Y9nHO"
   },
   "source": [
    "#### **1.1.1 Descripción de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripción sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulación en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5i1Wt1p770x"
   },
   "source": [
    "`escriba su respuesta acá`\n",
    "\n",
    "El entorno Blackjack es un juego de cartas modelado como un problema de decisión secuencial, donde el objetivo es maximizar la recompensa jugando contra un crupier. El jugador debe tomar decisiones en cada estado basándose en su mano, la carta visible del crupier y la posibilidad de usar un as como 11. Este entorno se considera un Problema de Decisión de Markov (MDP) porque satisface la propiedad de memoria de Markov: el siguiente estado y recompensa dependen únicamente del estado actual y la acción tomada.\n",
    "\n",
    "El espacio de acción está representado como un rango de valores discretos, {0, 1}, que indican las dos decisiones posibles del jugador. La acción 0 corresponde a plantarse (\"Palo\"), terminando el turno del jugador y permitiendo que el crupier actúe. Por otro lado, la acción 1 representa pedir una carta adicional (\"Golpe\"), lo que incrementa la suma de las cartas del jugador, con el riesgo de superar 21.\n",
    "\n",
    "El espacio de observación está definido como una tupla de tres elementos: la suma actual de las cartas del jugador, el valor de la carta visible del crupier (un número entre 1 y 10, donde 1 representa un as) y un indicador binario que señala si el jugador tiene un as utilizable (es decir, si el as puede contar como 11 sin exceder 21). Este diseño permite al jugador tomar decisiones estratégicas basadas en el estado del juego.\n",
    "\n",
    "Las recompensas en este entorno reflejan los resultados del juego: el jugador recibe +1 si gana, -1 si pierde y 0 en caso de empate. Si el jugador gana con un blackjack natural (un as y una carta de valor 10 como mano inicial), puede recibir una recompensa adicional de +1.5 si esta regla está habilitada. De lo contrario, la recompensa es de +1.\n",
    "\n",
    "Un episodio termina cuando el jugador elige pedir carta y su mano supera 21, o cuando decide plantarse, momento en el cual el crupier juega y se determina el resultado final. Cabe destacar que un as siempre se considerará utilizable (como 11) a menos que hacerlo cause que el jugador supere 21. Esto asegura que el juego se ajuste a las reglas del blackjack estándar y facilite la implementación del entorno como un MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmcX6bRC9agQ"
   },
   "source": [
    "#### **1.1.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulación 5000 veces y reporte el promedio y desviación de las recompensas. ¿Cómo calificaría el performance de esta política? ¿Cómo podría interpretar las recompensas obtenidas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9p2PrLLR9yju",
    "outputId": "c6e0a781-2032-46a9-8240-f598489ba9b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensas: -0.3746\n",
      "Desviación estándar de las recompensas: 0.9048065207545755\n"
     ]
    }
   ],
   "source": [
    "total_rewards = [] # recompensas\n",
    "\n",
    "# Repetir la simulación 5000 veces\n",
    "for _ in range(5000):\n",
    "    # Reiniciar el entorno\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Elegir una acción aleatoria (el espacio de acción tiene 2 dimensiones: \"acción 0\" o \"acción 1\")\n",
    "        action = env.action_space.sample()  # Esto selecciona una acción aleatoria\n",
    "\n",
    "        # Tomar la acción en el entorno\n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # Acumular la recompensa\n",
    "        total_reward += reward\n",
    "\n",
    "    # Guardar la recompensa total del episodio\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "# Convertir la lista de recompensas a un array de numpy para análisis\n",
    "total_rewards = np.array(total_rewards)\n",
    "\n",
    "# Calcular el promedio y desviación estándar\n",
    "average_reward = np.mean(total_rewards)\n",
    "std_deviation = np.std(total_rewards)\n",
    "\n",
    "# Reportar los resultados\n",
    "print(f\"Promedio de recompensas: {average_reward}\")\n",
    "print(f\"Desviación estándar de las recompensas: {std_deviation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEF8x4dLZVeR"
   },
   "source": [
    "Es mala perfomance porque la estrategia siempre entrega pérdida en valor esperado. El juego aleatorio estaría cargado en contra del jugador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEO_dY4x_SJu"
   },
   "source": [
    "#### **1.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9JsFA1wGmnH",
    "outputId": "e1a63bb5-6a7f-4940-f6de-09cc37fa2d1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.14     |\n",
      "|    ep_rew_mean        | -0.27    |\n",
      "| time/                 |          |\n",
      "|    fps                | 154      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.287   |\n",
      "|    explained_variance | 0.409    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.589    |\n",
      "|    value_loss         | 0.423    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03     |\n",
      "|    ep_rew_mean        | -0.15    |\n",
      "| time/                 |          |\n",
      "|    fps                | 207      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.182   |\n",
      "|    explained_variance | 0.777    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.00224  |\n",
      "|    value_loss         | 0.215    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.05      |\n",
      "|    ep_rew_mean        | -0.27     |\n",
      "| time/                 |           |\n",
      "|    fps                | 212       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.147    |\n",
      "|    explained_variance | 0.163     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -0.000519 |\n",
      "|    value_loss         | 0.804     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1        |\n",
      "|    ep_rew_mean        | -0.12    |\n",
      "| time/                 |          |\n",
      "|    fps                | 224      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0654  |\n",
      "|    explained_variance | 0.122    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.00071 |\n",
      "|    value_loss         | 0.843    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02     |\n",
      "|    ep_rew_mean        | -0.03    |\n",
      "| time/                 |          |\n",
      "|    fps                | 242      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0319  |\n",
      "|    explained_variance | 0.37     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.00198  |\n",
      "|    value_loss         | 0.681    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01     |\n",
      "|    ep_rew_mean        | -0.33    |\n",
      "| time/                 |          |\n",
      "|    fps                | 257      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0877  |\n",
      "|    explained_variance | -0.199   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.0172   |\n",
      "|    value_loss         | 1.49     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01     |\n",
      "|    ep_rew_mean        | -0.02    |\n",
      "| time/                 |          |\n",
      "|    fps                | 268      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0326  |\n",
      "|    explained_variance | 0.861    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.00223 |\n",
      "|    value_loss         | 0.232    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1        |\n",
      "|    ep_rew_mean        | -0.11    |\n",
      "| time/                 |          |\n",
      "|    fps                | 276      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0422  |\n",
      "|    explained_variance | -0.187   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.00325 |\n",
      "|    value_loss         | 0.95     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1        |\n",
      "|    ep_rew_mean        | -0.14    |\n",
      "| time/                 |          |\n",
      "|    fps                | 285      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.039   |\n",
      "|    explained_variance | 0.0699   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.00129  |\n",
      "|    value_loss         | 1.41     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01     |\n",
      "|    ep_rew_mean        | -0.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 291      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 17       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0298  |\n",
      "|    explained_variance | -0.0323  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -0.00329 |\n",
      "|    value_loss         | 0.923    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1        |\n",
      "|    ep_rew_mean        | -0.22    |\n",
      "| time/                 |          |\n",
      "|    fps                | 289      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0175  |\n",
      "|    explained_variance | 0.341    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.00211  |\n",
      "|    value_loss         | 0.838    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1        |\n",
      "|    ep_rew_mean        | -0.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 290      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0344  |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.00359 |\n",
      "|    value_loss         | 0.283    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01     |\n",
      "|    ep_rew_mean        | -0.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 296      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0277  |\n",
      "|    explained_variance | -0.331   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -0.00235 |\n",
      "|    value_loss         | 0.865    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.01      |\n",
      "|    ep_rew_mean        | -0.2      |\n",
      "| time/                 |           |\n",
      "|    fps                | 300       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0331   |\n",
      "|    explained_variance | 0.302     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -4.47e-05 |\n",
      "|    value_loss         | 0.674     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04     |\n",
      "|    ep_rew_mean        | -0.04    |\n",
      "| time/                 |          |\n",
      "|    fps                | 303      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 24       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0341  |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.00154 |\n",
      "|    value_loss         | 0.19     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02     |\n",
      "|    ep_rew_mean        | -0.14    |\n",
      "| time/                 |          |\n",
      "|    fps                | 306      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 26       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0968  |\n",
      "|    explained_variance | -0.0074  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.0495   |\n",
      "|    value_loss         | 1.19     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.03      |\n",
      "|    ep_rew_mean        | -0.29     |\n",
      "| time/                 |           |\n",
      "|    fps                | 310       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0247   |\n",
      "|    explained_variance | 0.0856    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -0.000534 |\n",
      "|    value_loss         | 0.588     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1        |\n",
      "|    ep_rew_mean        | -0.09    |\n",
      "| time/                 |          |\n",
      "|    fps                | 313      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 28       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0165  |\n",
      "|    explained_variance | 0.0529   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.000439 |\n",
      "|    value_loss         | 0.958    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03     |\n",
      "|    ep_rew_mean        | -0.13    |\n",
      "| time/                 |          |\n",
      "|    fps                | 314      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 30       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0206  |\n",
      "|    explained_variance | 0.454    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.000165 |\n",
      "|    value_loss         | 0.583    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04     |\n",
      "|    ep_rew_mean        | -0.14    |\n",
      "| time/                 |          |\n",
      "|    fps                | 311      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 32       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.316   |\n",
      "|    explained_variance | 0.537    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -0.063   |\n",
      "|    value_loss         | 0.445    |\n",
      "------------------------------------\n",
      "Episodio 1 - Recompensa Total: 1.0\n",
      "Episodio 2 - Recompensa Total: -1.0\n",
      "Episodio 3 - Recompensa Total: -1.0\n",
      "Episodio 4 - Recompensa Total: -1.0\n",
      "Episodio 5 - Recompensa Total: -1.0\n",
      "Episodio 6 - Recompensa Total: 1.0\n",
      "Episodio 7 - Recompensa Total: -1.0\n",
      "Episodio 8 - Recompensa Total: -1.0\n",
      "Episodio 9 - Recompensa Total: -1.0\n",
      "Episodio 10 - Recompensa Total: 1.0\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import A2C\n",
    "\n",
    "# Crear el modelo A2C con una red neuronal de 64 neuronas en una capa oculta\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Entrenar el modelo (10000 pasos de entrenamiento)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Evaluar el modelo en 10 episodios\n",
    "for episode in range(10):\n",
    "    obs, info = env.reset()  # Reiniciar el entorno\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        # Elegir una acción usando el modelo entrenado\n",
    "        action, _states = model.predict(obs)\n",
    "\n",
    "        # Ejecutar la acción en el entorno\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # Acumular la recompensa\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episodio {episode + 1} - Recompensa Total: {total_reward}\")\n",
    "\n",
    "# Guardar el modelo entrenado (opcional)\n",
    "model.save(\"a2c_blackjack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-bpdb8wZID1"
   },
   "source": [
    "#### **1.1.4 Evaluación de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.1.2 pero utilizando el modelo entrenado. ¿Cómo es el performance de su agente? ¿Es mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-d7d8GFf7F6",
    "outputId": "b80c043b-0d5a-4a9d-8a42-a35eb64a618e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensas: -0.1726\n",
      "Desviación estándar de las recompensas: 0.957710415522354\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo A2C entrenado (si lo tienes guardado)\n",
    "#model = A2C.load(\"a2c_blackjack\")\n",
    "\n",
    "# Lista para almacenar las recompensas\n",
    "total_rewards = []\n",
    "\n",
    "# Repetir la simulación 5000 veces\n",
    "for _ in range(5000):\n",
    "    # Reiniciar el entorno\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Elegir una acción usando el modelo entrenado\n",
    "        action, _states = model.predict(observation)\n",
    "\n",
    "        # Tomar la acción en el entorno\n",
    "        observation, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "        # Acumular la recompensa\n",
    "        total_reward += reward\n",
    "\n",
    "    # Guardar la recompensa total del episodio\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "# Convertir la lista de recompensas a un array de numpy para análisis\n",
    "total_rewards = np.array(total_rewards)\n",
    "\n",
    "# Calcular el promedio y desviación estándar\n",
    "average_reward = np.mean(total_rewards)\n",
    "std_deviation = np.std(total_rewards)\n",
    "\n",
    "# Reportar los resultados\n",
    "print(f\"Promedio de recompensas: {average_reward}\")\n",
    "print(f\"Desviación estándar de las recompensas: {std_deviation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iqg4ygJkbkHY"
   },
   "source": [
    "Mantiene baja performance pero mejora respecto al baseline. Se obtiene un promedio de pérdidas menor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO-EsAaPAYEm"
   },
   "source": [
    "#### **1.1.5 Estudio de acciones (0.2 puntos)**\n",
    "\n",
    "Genere una función que reciba un estado y retorne la accion del agente. Luego, use esta función para entregar la acción escogida frente a los siguientes escenarios:\n",
    "\n",
    "- Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
    "- Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
    "\n",
    "¿Son coherentes sus acciones con las reglas del juego?\n",
    "\n",
    "Hint: ¿A que clase de python pertenecen los estados? Pruebe a usar el método `.reset` para saberlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8BnvgONQcTC7",
    "outputId": "abb9f4fe-2d68-42da-981d-147b3e6265cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([15, 10,  1]), {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fh8XlGyzwtRp",
    "outputId": "f371968e-ae3f-4e6a-c8c7-3830a80ef29f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acción en el escenario 1 (Agente suma 6, Dealer muestra 7, sin as): 0\n",
      "Acción en el escenario 2 (Agente suma 19, Dealer muestra 3, con as): 0\n"
     ]
    }
   ],
   "source": [
    "# Función para obtener la acción del agente dado un estado\n",
    "def obtener_accion(state):\n",
    "    # Usamos el modelo entrenado para predecir la acción en base al estado\n",
    "    action, _states = model.predict(state)\n",
    "    return action\n",
    "\n",
    "# Primer escenario: Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene un as\n",
    "# Para simular este escenario, creamos un estado artificialmente. Suponemos que el estado es un vector de numpy.\n",
    "# Nota: La representación del estado dependerá de cómo esté estructurado el entorno. Para fines de este ejercicio,\n",
    "# asumimos que el estado es un array que contiene [suma_agente, suma_dealer, tiene_as].\n",
    "# Este es un ejemplo simplificado.\n",
    "\n",
    "estado_escenario_1 = np.array([6, 7, 0])  # Suma del agente es 6, dealer muestra 7, agente no tiene un as\n",
    "accion_1 = obtener_accion(estado_escenario_1)\n",
    "\n",
    "# Segundo escenario: Suma de cartas del agente es 19, dealer muestra un 3, agente tiene un as\n",
    "estado_escenario_2 = np.array([19, 3, 1])  # Suma del agente es 19, dealer muestra 3, agente tiene un as\n",
    "accion_2 = obtener_accion(estado_escenario_2)\n",
    "\n",
    "# Imprimir las acciones en ambos escenarios\n",
    "print(f\"Acción en el escenario 1 (Agente suma 6, Dealer muestra 7, sin as): {accion_1}\")\n",
    "print(f\"Acción en el escenario 2 (Agente suma 19, Dealer muestra 3, con as): {accion_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEqCTqqroh03"
   },
   "source": [
    "### **1.2 LunarLander**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.redd.it/097t6tk29zf51.jpg\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la sección 2.1, en esta sección usted se encargará de implementar una gente de RL que pueda resolver el ambiente `LunarLander`.\n",
    "\n",
    "Comencemos preparando el ambiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nvQUyuZ_FtZ4",
    "outputId": "603d8263-e93d-4ee6-9550-94aa2b5b3a36"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\", continuous = True) # notar el parámetro continuous = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBU4lGX3wpN6"
   },
   "source": [
    "Noten que se especifica el parámetro `continuous = True`. ¿Que implicancias tiene esto sobre el ambiente?\n",
    "\n",
    "Además, se le facilita la función `export_gif` para el ejercicio 2.2.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRiWpSo9yfr9"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "def export_gif(model, n = 5):\n",
    "  '''\n",
    "  función que exporta a gif el comportamiento del agente en n episodios\n",
    "  '''\n",
    "  images = []\n",
    "  for episode in range(n):\n",
    "    obs = model.env.reset()\n",
    "    img = model.env.render()\n",
    "    done = False\n",
    "    while not done:\n",
    "      images.append(img)\n",
    "      action, _ = model.predict(obs)\n",
    "      obs, reward, done, info = model.env.step(action)\n",
    "      img = model.env.render(mode=\"rgb_array\")\n",
    "\n",
    "  imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk5VJVppXh3N"
   },
   "source": [
    "#### **1.2.1 Descripción de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripción sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulación en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas. ¿Como se distinguen las acciones de este ambiente en comparación a `Blackjack`?\n",
    "\n",
    "Nota: recuerde que se especificó el parámetro `continuous = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb-u9LUE8O9a"
   },
   "source": [
    "`escriba su respuesta acá`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YChodtNQwzG2"
   },
   "source": [
    "#### **1.2.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulación 10 veces y reporte el promedio y desviación de las recompensas. ¿Cómo calificaría el performance de esta política?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bwc3A0GX7a8",
    "outputId": "641020ad-b365-4181-f3ab-2fbe7912d3d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensas: -210.40957651492937\n",
      "Desviación estándar de las recompensas: 156.8861580778027\n"
     ]
    }
   ],
   "source": [
    "# Lista para almacenar las recompensas totales de cada episodio\n",
    "rewards = []\n",
    "\n",
    "# Ejecutar la simulación 10 veces con acciones aleatorias\n",
    "for episode in range(10):\n",
    "    obs, info = env.reset()  # Reiniciar el entorno\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Seleccionar una acción aleatoria\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Tomar la acción en el entorno\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # Acumular la recompensa total\n",
    "        total_reward += reward\n",
    "\n",
    "    # Guardar la recompensa total del episodio\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# Convertir la lista de recompensas a un array de numpy para análisis\n",
    "rewards = np.array(rewards)\n",
    "\n",
    "# Calcular el promedio y desviación estándar de las recompensas\n",
    "average_reward = np.mean(rewards)\n",
    "std_deviation = np.std(rewards)\n",
    "\n",
    "# Reportar los resultados\n",
    "print(f\"Promedio de recompensas: {average_reward}\")\n",
    "print(f\"Desviación estándar de las recompensas: {std_deviation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDENs-lRc4l7"
   },
   "source": [
    "Al igual que el baseline del punto anterior, en promedio se obtiene pérdidas al seguir esta estrategia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQrZVQflX_5f"
   },
   "source": [
    "#### **1.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_6Ia9uoF7Hs",
    "outputId": "cf3137df-bfc1-4ea0-bf5e-a0ede179995c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.2     |\n",
      "|    ep_rew_mean     | -721     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 373      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.07     |\n",
      "|    critic_loss     | 69       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 272      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93       |\n",
      "|    ep_rew_mean     | -937     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 744      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.6     |\n",
      "|    critic_loss     | 35.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 643      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.7     |\n",
      "|    ep_rew_mean     | -976     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1088     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 32.3     |\n",
      "|    critic_loss     | 7.81     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 987      |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 90.8      |\n",
      "|    ep_rew_mean     | -1.03e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 158       |\n",
      "|    time_elapsed    | 9         |\n",
      "|    total_timesteps | 1452      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 41.8      |\n",
      "|    critic_loss     | 10.1      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 1351      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 91.8      |\n",
      "|    ep_rew_mean     | -1.08e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 159       |\n",
      "|    time_elapsed    | 11        |\n",
      "|    total_timesteps | 1837      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 55.3      |\n",
      "|    critic_loss     | 18.7      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 1736      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 93.2      |\n",
      "|    ep_rew_mean     | -1.11e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 159       |\n",
      "|    time_elapsed    | 13        |\n",
      "|    total_timesteps | 2237      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 59.5      |\n",
      "|    critic_loss     | 106       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 2136      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 91.2      |\n",
      "|    ep_rew_mean     | -1.11e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 153       |\n",
      "|    time_elapsed    | 16        |\n",
      "|    total_timesteps | 2553      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 69.6      |\n",
      "|    critic_loss     | 53.4      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 2452      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 91        |\n",
      "|    ep_rew_mean     | -1.13e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 32        |\n",
      "|    fps             | 153       |\n",
      "|    time_elapsed    | 18        |\n",
      "|    total_timesteps | 2911      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 82.2      |\n",
      "|    critic_loss     | 87        |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 2810      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 91.2      |\n",
      "|    ep_rew_mean     | -1.14e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 36        |\n",
      "|    fps             | 154       |\n",
      "|    time_elapsed    | 21        |\n",
      "|    total_timesteps | 3285      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 87.2      |\n",
      "|    critic_loss     | 84.7      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 3184      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 90.9      |\n",
      "|    ep_rew_mean     | -1.13e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 40        |\n",
      "|    fps             | 155       |\n",
      "|    time_elapsed    | 23        |\n",
      "|    total_timesteps | 3636      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 94        |\n",
      "|    critic_loss     | 211       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 3535      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 90.5      |\n",
      "|    ep_rew_mean     | -1.13e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 44        |\n",
      "|    fps             | 156       |\n",
      "|    time_elapsed    | 25        |\n",
      "|    total_timesteps | 3980      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 103       |\n",
      "|    critic_loss     | 119       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 3879      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 89.5      |\n",
      "|    ep_rew_mean     | -1.11e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 48        |\n",
      "|    fps             | 155       |\n",
      "|    time_elapsed    | 27        |\n",
      "|    total_timesteps | 4295      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 102       |\n",
      "|    critic_loss     | 115       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 4194      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 89.2      |\n",
      "|    ep_rew_mean     | -1.11e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 52        |\n",
      "|    fps             | 152       |\n",
      "|    time_elapsed    | 30        |\n",
      "|    total_timesteps | 4637      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 111       |\n",
      "|    critic_loss     | 271       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 4536      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.4     |\n",
      "|    ep_rew_mean     | -1.1e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 4949     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 113      |\n",
      "|    critic_loss     | 258      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4848     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.5     |\n",
      "|    ep_rew_mean     | -1.1e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 5313     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 118      |\n",
      "|    critic_loss     | 183      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5212     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.4     |\n",
      "|    ep_rew_mean     | -1.1e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 5660     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 113      |\n",
      "|    critic_loss     | 326      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5559     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 87.6      |\n",
      "|    ep_rew_mean     | -1.09e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 68        |\n",
      "|    fps             | 154       |\n",
      "|    time_elapsed    | 38        |\n",
      "|    total_timesteps | 5955      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 122       |\n",
      "|    critic_loss     | 217       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 5854      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 87.6      |\n",
      "|    ep_rew_mean     | -1.09e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 72        |\n",
      "|    fps             | 154       |\n",
      "|    time_elapsed    | 40        |\n",
      "|    total_timesteps | 6308      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 129       |\n",
      "|    critic_loss     | 317       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 6207      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 87.2      |\n",
      "|    ep_rew_mean     | -1.08e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 76        |\n",
      "|    fps             | 152       |\n",
      "|    time_elapsed    | 43        |\n",
      "|    total_timesteps | 6629      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 138       |\n",
      "|    critic_loss     | 247       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 6528      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 87.5      |\n",
      "|    ep_rew_mean     | -1.09e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 80        |\n",
      "|    fps             | 153       |\n",
      "|    time_elapsed    | 45        |\n",
      "|    total_timesteps | 6998      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -10.4     |\n",
      "|    critic_loss     | 234       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 6897      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 86.5      |\n",
      "|    ep_rew_mean     | -1.05e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 84        |\n",
      "|    fps             | 153       |\n",
      "|    time_elapsed    | 47        |\n",
      "|    total_timesteps | 7270      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.61     |\n",
      "|    critic_loss     | 497       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 7169      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 86.3      |\n",
      "|    ep_rew_mean     | -1.02e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 88        |\n",
      "|    fps             | 154       |\n",
      "|    time_elapsed    | 49        |\n",
      "|    total_timesteps | 7597      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02     |\n",
      "|    critic_loss     | 677       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 7496      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.9     |\n",
      "|    ep_rew_mean     | -999     |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 7997     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.63     |\n",
      "|    critic_loss     | 163      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7896     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.3     |\n",
      "|    ep_rew_mean     | -974     |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 8475     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.8     |\n",
      "|    critic_loss     | 157      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8374     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.8     |\n",
      "|    ep_rew_mean     | -944     |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 8882     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 28.3     |\n",
      "|    critic_loss     | 125      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8781     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89.6     |\n",
      "|    ep_rew_mean     | -936     |\n",
      "| time/              |          |\n",
      "|    episodes        | 104      |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 61       |\n",
      "|    total_timesteps | 9334     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 29       |\n",
      "|    critic_loss     | 48.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9233     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.8     |\n",
      "|    ep_rew_mean     | -904     |\n",
      "| time/              |          |\n",
      "|    episodes        | 108      |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 64       |\n",
      "|    total_timesteps | 9824     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 36.4     |\n",
      "|    critic_loss     | 34.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9723     |\n",
      "---------------------------------\n",
      "Episodio 1 - Recompensa Total: -524.0027137816859\n",
      "Episodio 2 - Recompensa Total: -572.4100436709303\n",
      "Episodio 3 - Recompensa Total: -430.4101743437193\n",
      "Episodio 4 - Recompensa Total: -55.04549564251315\n",
      "Episodio 5 - Recompensa Total: -230.24836993726544\n",
      "Episodio 6 - Recompensa Total: -440.8368314316917\n",
      "Episodio 7 - Recompensa Total: -88.98140390626085\n",
      "Episodio 8 - Recompensa Total: -500.9831978302545\n",
      "Episodio 9 - Recompensa Total: -492.1418546399306\n",
      "Episodio 10 - Recompensa Total: -243.72989140572906\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import TD3\n",
    "\n",
    "# Crear el modelo A2C con una red neuronal de 64 neuronas en una capa oculta\n",
    "model2 = TD3(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Entrenar el modelo (10000 pasos de entrenamiento)\n",
    "model2.learn(total_timesteps=10000)\n",
    "\n",
    "# Evaluar el modelo en 10 episodios\n",
    "for episode in range(10):\n",
    "    obs, info = env.reset()  # Reiniciar el entorno\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        # Elegir una acción usando el modelo entrenado\n",
    "        action, _states = model2.predict(obs)\n",
    "\n",
    "        # Ejecutar la acción en el entorno\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # Acumular la recompensa\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episodio {episode + 1} - Recompensa Total: {total_reward}\")\n",
    "\n",
    "# Guardar el modelo entrenado (opcional)\n",
    "model2.save(\"td2_LunarLander\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z-oIUSrlAsY"
   },
   "source": [
    "#### **1.2.4 Evaluación de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.2.2 pero utilizando el modelo entrenado. ¿Cómo es el performance de su agente? ¿Es mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ophyU3KrWrwl",
    "outputId": "98009c8f-d810-4522-c546-eebf462b2542"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensas: -286.66797663417617\n",
      "Desviación estándar de las recompensas: 139.41690965104962\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo A2C entrenado (si lo tienes guardado)\n",
    "#model2 = TD3.load(\"td2_LunarLander\")\n",
    "\n",
    "# Lista para almacenar las recompensas\n",
    "total_rewards = []\n",
    "\n",
    "# Repetir la simulación 5000 veces\n",
    "for _ in range(5000):\n",
    "    # Reiniciar el entorno\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Elegir una acción usando el modelo entrenado\n",
    "        action, _states = model2.predict(observation)\n",
    "\n",
    "        # Tomar la acción en el entorno\n",
    "        observation, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "        # Acumular la recompensa\n",
    "        total_reward += reward\n",
    "\n",
    "    # Guardar la recompensa total del episodio\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "# Convertir la lista de recompensas a un array de numpy para análisis\n",
    "total_rewards = np.array(total_rewards)\n",
    "\n",
    "# Calcular el promedio y desviación estándar\n",
    "average_reward = np.mean(total_rewards)\n",
    "std_deviation = np.std(total_rewards)\n",
    "\n",
    "# Reportar los resultados\n",
    "print(f\"Promedio de recompensas: {average_reward}\")\n",
    "print(f\"Desviación estándar de las recompensas: {std_deviation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6Xw4YHT3P5d"
   },
   "source": [
    "#### **1.2.5 Optimización de modelo (0.2 puntos)**\n",
    "\n",
    "Repita los ejercicios 1.2.3 y 1.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente parámetros como:\n",
    "- `total_timesteps`\n",
    "- `learning_rate`\n",
    "- `batch_size`\n",
    "\n",
    "Una vez optimizado el modelo, use la función `export_gif` para estudiar el comportamiento de su agente en la resolución del ambiente y comente sobre sus resultados.\n",
    "\n",
    "Adjunte el gif generado en su entrega (mejor aún si además adjuntan el gif en el markdown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "aItYF6sr6F_6",
    "outputId": "5b3ec38c-d067-4ff6-f78e-b3a995966a02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "IsLocked() == false",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-537aa530a9ad>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Entrenar el modelo (10,000 pasos de entrenamiento)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Lista para almacenar las recompensas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/td3/td3.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     ) -> SelfTD3:\n\u001b[0;32m--> 222\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     ) -> SelfOffPolicyAlgorithm:\n\u001b[0;32m--> 314\u001b[0;31m         total_timesteps, callback = self._setup_learn(\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36m_setup_learn\u001b[0;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorizedActionNoise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         return super()._setup_learn(\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36m_setup_learn\u001b[0;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_num_timesteps\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_obs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_episode_starts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0;31m# Retrieve unnormalized observation for saving into the buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mmaybe_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"options\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_seeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmaybe_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Seeds and options are only used once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected you to pass keyword argument {key} into reset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_reset_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mActType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mObsType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSupportsFloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \"\"\"\n\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    326\u001b[0m     ) -> tuple[WrapperObsType, dict[str, Any]]:\n\u001b[1;32m    327\u001b[0m         \u001b[0;34m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRenderFrame\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRenderFrame\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;34m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRenderFrame\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRenderFrame\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    326\u001b[0m     ) -> tuple[WrapperObsType, dict[str, Any]]:\n\u001b[1;32m    327\u001b[0m         \u001b[0;34m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRenderFrame\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRenderFrame\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_reset_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRenderFrame\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRenderFrame\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    328\u001b[0m     ):\n\u001b[1;32m    329\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_destroy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;31m# Bug's workaround for: https://github.com/Farama-Foundation/Gymnasium/issues/728\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36m_destroy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontactListener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_particles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDestroyBody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36m_clean_particles\u001b[0;34m(self, all_particle)\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_clean_particles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_particle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticles\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mall_particle\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mttl\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDestroyBody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: IsLocked() == false"
     ]
    }
   ],
   "source": [
    "# Crear el modelo con parámetros personalizados\n",
    "model2 = TD3(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=0.001,   # Ajusta la tasa de aprendizaje\n",
    "    batch_size=16,         # Ajusta el tamaño de los lotes\n",
    "    verbose=1              # Nivel de detalle en los logs\n",
    ")\n",
    "\n",
    "# Entrenar el modelo (10,000 pasos de entrenamiento)\n",
    "model2.learn(total_timesteps=10000)\n",
    "\n",
    "# Lista para almacenar las recompensas\n",
    "total_rewards = []\n",
    "\n",
    "# Repetir la simulación 5000 veces\n",
    "for _ in range(5000):\n",
    "    # Reiniciar el entorno\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Elegir una acción usando el modelo entrenado\n",
    "        action, _states = model2.predict(observation)\n",
    "\n",
    "        # Tomar la acción en el entorno\n",
    "        observation, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "        # Acumular la recompensa\n",
    "        total_reward += reward\n",
    "\n",
    "    # Guardar la recompensa total del episodio\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "# Convertir la lista de recompensas a un array de numpy para análisis\n",
    "total_rewards = np.array(total_rewards)\n",
    "\n",
    "# Calcular el promedio y desviación estándar\n",
    "average_reward = np.mean(total_rewards)\n",
    "std_deviation = np.std(total_rewards)\n",
    "\n",
    "# Reportar los resultados\n",
    "print(f\"Promedio de recompensas: {average_reward}\")\n",
    "print(f\"Desviación estándar de las recompensas: {std_deviation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPUY-Ktgf2BO"
   },
   "source": [
    "## **2. Large Language Models (4.0 puntos)**\n",
    "\n",
    "En esta sección se enfocarán en habilitar un Chatbot que nos permita responder preguntas útiles a través de LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ4fPRRihGLe"
   },
   "source": [
    "### **2.0 Configuración Inicial**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/uqAs9atZH58AAAAd/config-config-issue.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Como siempre, cargamos todas nuestras API KEY al entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ud2Xm_k-hFJn",
    "outputId": "f08dcb8b-2dbe-46c7-8a13-e0454abcf553"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Google AI API key: ··········\n",
      "Enter your Tavily API key: ··········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj9JvQUsgZZJ"
   },
   "source": [
    "### **2.1 Retrieval Augmented Generation (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://y.yarn.co/218aaa02-c47e-4ec9-b1c9-07792a06a88f_text.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsección es que habiliten un chatbot que pueda responder preguntas usando información contenida en documentos PDF a través de **Retrieval Augmented Generation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrxOQroVnaZ5"
   },
   "source": [
    "#### **2.1.1 Reunir Documentos (0 puntos)**\n",
    "\n",
    "Reuna documentos PDF sobre los que hacer preguntas siguiendo las siguientes instrucciones:\n",
    "  - 2 documentos .pdf como mínimo.\n",
    "  - 50 páginas de contenido como mínimo entre todos los documentos.\n",
    "  - Ideas para documentos: Documentos relacionados a temas académicos, laborales o de ocio. Aprovechen este ejercicio para construir algo útil y/o relevante para ustedes!\n",
    "  - Deben ocupar documentos reales, no pueden utilizar los mismos de la clase.\n",
    "  - Deben registrar sus documentos en la siguiente [planilla](https://docs.google.com/spreadsheets/d/1Hy1w_dOiG2UCHJ8muyxhdKPZEPrrL7BNHm6E90imIIM/edit?usp=sharing). **NO PUEDEN USAR LOS MISMOS DOCUMENTOS QUE OTRO GRUPO**\n",
    "  - **Recuerden adjuntar los documentos en su entrega**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5D1tIRCi4oJJ",
    "outputId": "5cf17d37-14a4-47c1-f728-cde913aa389e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzq2TjWCnu15"
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "doc_paths = [\"prevencionycontrol.pdf\", \"ratasenpropiedad.pdf\",\"ratasyratones.pdf\"] \n",
    "\n",
    "assert len(doc_paths) >= 2, \"Deben adjuntar un mínimo de 2 documentos\"\n",
    "\n",
    "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in doc_paths)\n",
    "assert total_paginas >= 50, f\"Páginas insuficientes: {total_paginas}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r811-P71nizA"
   },
   "source": [
    "#### **2.1.2 Vectorizar Documentos (0.2 puntos)**\n",
    "\n",
    "Vectorice los documentos y almacene sus representaciones de manera acorde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2KZ_wK7xw8Zg",
    "outputId": "b58795bc-5e54-4cff-d200-07608408ade3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --quiet faiss-cpu langchain_community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xseuWa5oyEQu",
    "outputId": "9ee351e1-0b7f-4c24-fb62-2683d76d392a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_google_genai\n",
      "  Downloading langchain_google_genai-2.0.5-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (0.8.3)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (0.3.19)\n",
      "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (2.9.2)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.6.10)\n",
      "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.19.2)\n",
      "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.151.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.27.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.25.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.25.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain_google_genai) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain_google_genai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain_google_genai) (0.1.143)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain_google_genai) (24.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain_google_genai) (9.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.23.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.66.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.9)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.15->langchain_google_genai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain_google_genai) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain_google_genai) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain_google_genai) (1.0.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.1.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.68.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.62.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (3.2.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain_google_genai) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain_google_genai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain_google_genai) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain_google_genai) (3.10)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain_google_genai) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain_google_genai) (0.14.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.2.3)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain_google_genai) (1.2.2)\n",
      "Downloading langchain_google_genai-2.0.5-py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langchain_google_genai\n",
      "Successfully installed langchain_google_genai-2.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_google_genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-yXAdCSn4JM",
    "outputId": "801f385a-7199-4264-f16c-c3fbb7d4b2c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/google/colab/html/_background_server.py:103: DeprecationWarning: make_current is deprecated; start the event loop first\n",
      "  ioloop.make_current()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7dd83cdd30a0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Inicializar el cargador de documentos y cargar cada documento\n",
    "documents = []\n",
    "for path in doc_paths:\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents.extend(loader.load())  # Cargar documentos y añadirlos a la lista\n",
    "\n",
    "# Inicializar el divisor de texto\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(documents)  # Dividir documentos en chunks\n",
    "\n",
    "# Inicializar embeddings\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Crear vectorstore y vectorizar los documentos\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embedding)\n",
    "\n",
    "# Mostrar un resumen del vectorstore\n",
    "vectorstore\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAUkP5zrnyBK"
   },
   "source": [
    "#### **2.1.3 Habilitar RAG (0.3 puntos)**\n",
    "\n",
    "Habilite la solución RAG a través de una *chain* y guárdela en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "t4-RURZwBGVm"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", # modelo de lenguaje\n",
    "    temperature=0, # probabilidad de \"respuestas creativas\"\n",
    "    max_tokens=None, # sin tope de tokens\n",
    "    timeout=None, # sin timeout\n",
    "    max_retries=2, # número máximo de intentos\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "gPIySdDFn99l"
   },
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", # método de búsqueda\n",
    "                                     search_kwargs={\"k\": 3}, # n° documentos a recuperar\n",
    "                                     )\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "retriever_chain = retriever | format_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "PymbEGKKAPo_"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "rag_template = '''\n",
    "Eres un asistente experto en el cuidado del hogar con respecto al control de roedores como ratas y ratones.\n",
    "Tu único rol es contestar preguntas del usuario a partir de información relevante que te sea proporcionada.\n",
    "Responde siempre de la forma más completa posible y usando toda la información entregada.\n",
    "Responde sólo lo que te pregunten a partir de la información relevante, NUNCA inventes una respuesta.\n",
    "\n",
    "Información relevante: {context}\n",
    "Pregunta: {question}\n",
    "Respuesta útil:\n",
    "'''\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M2jBskJ9AgQ4",
    "outputId": "cf0a5427-95a5-42ef-b5ca-c2f500ad11f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La información proporcionada no contiene datos sobre \"nulos y blancos\".  Por lo tanto, no puedo responder a tu pregunta.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever_chain, # context lo obtendremos del retriever_chain\n",
    "        \"question\": RunnablePassthrough(), # question pasará directo hacia el prompt\n",
    "    }\n",
    "    | rag_prompt # prompt con las variables question y context\n",
    "    | llm # llm recibe el prompt y responde\n",
    "    | StrOutputParser() # recuperamos sólo la respuesta\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycg5S5i_n-kL"
   },
   "source": [
    "#### **2.1.4 Verificación de respuestas (0.5 puntos)**\n",
    "\n",
    "Genere un listado de 3 tuplas (\"pregunta\", \"respuesta correcta\") y analice la respuesta de su solución para cada una. ¿Su solución RAG entrega las respuestas que esperaba?\n",
    "\n",
    "Ejemplo de tupla:\n",
    "- Pregunta: ¿Quién es el presidente de Chile?\n",
    "- Respuesta correcta: El presidente de Chile es Gabriel Boric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S_UiEn1hoZYR",
    "outputId": "664206f0-917b-4c8b-8640-4471121fd6df"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: ¿Cómo elegir una empresa de control de plagas adecuada?\n",
      "Respuesta: Para elegir una empresa de control de plagas adecuada, debe seguir estos pasos:\n",
      "\n",
      "1. **Pregunte y entreviste a varias empresas:**  Obtenga información sobre sus servicios y experiencia.\n",
      "\n",
      "2. **Pida referencias:** Consulte con vecinos y amigos para conocer sus experiencias con diferentes empresas.\n",
      "\n",
      "3. **Busque información:** Revise directorios telefónicos e internet para encontrar empresas que ofrezcan \"control integral de plagas\".  Estas empresas suelen realizar revisiones, control, reparaciones y recomendaciones.\n",
      "\n",
      "\n",
      "\n",
      "Pregunta: ¿Qué pasa si se detectan ratas en la calle?\n",
      "Respuesta: Si se detectan ratas en la calle, la ASPB (se asume que es una agencia de control de plagas) actúa en la vía pública, la red de alcantarillado y otros espacios públicos como solares, equipamientos y mercados municipales.  Para reportarlo, se puede llamar al 010 o al 900 226 226, usar el Servicio de Atención en Línea, acudir a las Oficinas de Atención Ciudadana o usar la app “Barcelona en el bolsillo”.  Informar sobre la presencia de roedores es clave para mantener las plagas bajo control.\n",
      "\n",
      "\n",
      "\n",
      "Pregunta: ¿De que prefieren alimentarse las ratas?\n",
      "Respuesta: Las ratas prefieren alimentarse de aguacates, bayas, cítricos, semillas de pasto y para pájaros, hiedra, nueces, comida para mascotas, fruta madura y caracoles.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"¿Cómo elegir una empresa de control de plagas adecuada?\",\n",
    "    \"¿Qué pasa si se detectan ratas en la calle?\",\n",
    "    \"¿De que prefieren alimentarse las ratas?\"\n",
    "]\n",
    "\n",
    "# Iterar sobre las preguntas y obtener respuestas para cada una\n",
    "for query in questions:\n",
    "    response = rag_chain.invoke(query)  # Recupera la respuesta para cada pregunta\n",
    "    print(f\"Pregunta: {query}\")\n",
    "    print(f\"Respuesta: {response}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8d5zTMHoUgF"
   },
   "source": [
    "#### **2.1.5 Sensibilidad de Hiperparámetros (0.5 puntos)**\n",
    "\n",
    "Extienda el análisis del punto 2.1.4 analizando cómo cambian las respuestas entregadas cambiando los siguientes hiperparámetros:\n",
    "- `Tamaño del chunk`. (*¿Cómo repercute que los chunks sean mas grandes o chicos?*)\n",
    "- `La cantidad de chunks recuperados`. (*¿Qué pasa si se devuelven muchos/pocos chunks?*)\n",
    "- `El tipo de búsqueda`. (*¿Cómo afecta el tipo de búsqueda a las respuestas de mi RAG?*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UDh_QgeXLGHc",
    "outputId": "900b0eff-539f-4d89-e567-19469907426f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta con chunk_size 100:\n",
      "Para controlar la población de ratas, primero debe buscar evidencia de su presencia.  Luego, controle la maleza, los matorrales y los arbustos, ya que las ratas suelen vivir en madrigueras.\n",
      "\n",
      "\n",
      "Respuesta con chunk_size 500:\n",
      "Para controlar la población de ratas, debe eliminar todo lo que necesitan para sobrevivir: comida, agua, refugio y rutas de movilidad.  Esto implica controlar la maleza, los matorrales y los arbustos, ya que las ratas suelen vivir en madrigueras debajo de ellos.  Se debe evitar el césped alto, arbustos, matorrales y mantillo cerca de los cimientos de las construcciones.  Es necesario quitar la hiedra alrededor de las madrigueras, mantener un espacio descubierto de 6 pulgadas alrededor de las construcciones y podar debajo de los arbustos.  Finalmente, se debe conservar espacio entre las plantas y evitar la vegetación densa.\n",
      "\n",
      "\n",
      "Respuesta con chunk_size 1000:\n",
      "Para controlar la población de ratas, es fundamental un manejo adecuado del medio ambiente, enfocándose en eliminar sus tres necesidades básicas: alimento, agua y refugio.  Esto implica prácticas de higiene tanto en el hogar como en el entorno.  Específicamente, se recomienda cosechar y recoger diariamente frutas y nueces maduras; guardar la comida sobrante de las mascotas por la noche; mantener podadas las palmeras y plantas de yuca, y eliminar o podar la hiedra y otras plantas densas lejos de estructuras; reparar fugas de agua y eliminar el agua estancada.  Adicionalmente, se debe controlar la maleza, matorrales y arbustos, manteniendo un espacio descubierto de 6 pulgadas alrededor de las construcciones, podando debajo de los arbustos, conservando espacio entre las plantas y evitando la vegetación densa.  Se debe mantener los jardines libres de maleza y basura.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunk_sizes = [100, 500, 1000]  # Tamaños de chunk a probar\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=50)\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=embedding)\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "    retriever_chain = retriever | format_docs\n",
    "    rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever_chain, # context lo obtendremos del retriever_chain\n",
    "        \"question\": RunnablePassthrough(), # question pasará directo hacia el prompt\n",
    "    }\n",
    "    | rag_prompt # prompt con las variables question y context\n",
    "    | llm # llm recibe el prompt y responde\n",
    "    | StrOutputParser() # recuperamos sólo la respuesta\n",
    ")\n",
    "    # Ejecutar la búsqueda y analizar las respuestas\n",
    "    query = \"¿Cómo controlar la población de ratas?\"\n",
    "    response = rag_chain.invoke(query)\n",
    "    print(f\"Respuesta con chunk_size {chunk_size}:\\n{response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kTB8LpFs9hcU",
    "outputId": "66f5d0dd-9830-4fd3-b557-83f64a09bf7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta con k=1:\n",
      "Para controlar la población de ratas, es necesaria una prevención activa.  Esto se debe a que la presencia de ratas causa incomodidad, daños materiales y molestias.  Aunque en los últimos años no hay evidencia de transmisión de enfermedades de roedores a personas en la ciudad, las ratas pueden ser portadoras de enfermedades transmisibles, representando un riesgo para la salud.\n",
      "\n",
      "\n",
      "Respuesta con k=3:\n",
      "Para controlar la población de ratas, es fundamental un manejo adecuado del medio ambiente, enfocándose en eliminar sus tres necesidades básicas: alimento, agua y refugio.  Esto implica prácticas de higiene tanto en el hogar como en el entorno.  Específicamente, se recomienda cosechar y recoger diariamente frutas y nueces maduras; guardar la comida sobrante de las mascotas por la noche; mantener podadas las palmeras y plantas de yuca, y eliminar o podar la hiedra y otras plantas densas lejos de estructuras; reparar fugas de agua y eliminar el agua estancada.  Adicionalmente, se debe controlar la maleza, arbustos y césped alto alrededor de las construcciones, manteniendo un espacio descubierto de al menos 6 pulgadas entre la vegetación y los cimientos.  Es importante conservar espacio entre las plantas y evitar la vegetación densa para reducir la disponibilidad de refugio.\n",
      "\n",
      "\n",
      "Respuesta con k=5:\n",
      "Para controlar la población de ratas, se debe seguir un plan de cinco pasos:\n",
      "\n",
      "**Paso uno: Buscar evidencia:**  Identifique los lugares donde viven las ratas buscando madrigueras (agujeros en la tierra o cemento de 1 a 4 pulgadas de ancho con bordes lisos, generalmente debajo de arbustos y plantas, con agujeros de entrada y salida), excrementos (generalmente cerca de la basura, húmedo y oscuro indica presencia reciente), agujeros y roeduras en contenedores de basura, y senderos (marcas oscuras y grasosas en paredes y césped).\n",
      "\n",
      "**Paso dos: Limpiar:**  Quite excremento y huellas. Deshágase del desorden. Controle la maleza, los matorrales y los arbustos.  Esto implica mantener un espacio descubierto de 6 pulgadas alrededor de las construcciones, podar debajo de los arbustos, conservar espacio entre las plantas, evitar vegetación densa, y mantener los jardines libres de maleza y basura.\n",
      "\n",
      "**Paso tres: Privarlas de comida:** Controle su basura. Mantenga la comida alejada.  Esto incluye cosechar y recoger diariamente frutas y nueces maduras, no dejar comida de mascotas afuera durante la noche, y mantener podadas las palmeras y plantas de yuca, eliminando o podando la hiedra de Argelia y otras plantas densas lejos de techos, paredes, bardas, postes y árboles.\n",
      "\n",
      "**Paso cuatro: Cerrarles la puerta:** Selle las grietas y los agujeros pequeños. Tape agujeros y aberturas grandes. Cierre las madrigueras.\n",
      "\n",
      "**Paso cinco: Exterminarlas:**  (La información proporcionada no detalla métodos específicos de exterminio, solo menciona el cebo para roedores y la posibilidad de contratar una empresa de control de plagas).\n",
      "\n",
      "\n",
      "Respuesta con k=10:\n",
      "Para controlar la población de ratas, se debe seguir un proceso de cinco pasos:\n",
      "\n",
      "**Paso uno: Buscar evidencia.**  Identifique los lugares donde viven las ratas buscando madrigueras (agujeros en la tierra o cemento de 1 a 4 pulgadas de ancho con bordes lisos, generalmente con un agujero de entrada y salida), excremento (generalmente cerca de la basura, húmedo y oscuro indica presencia de ratas), agujeros y roeduras en contenedores de basura, y senderos (marcas oscuras y grasosas a lo largo de paredes y césped).\n",
      "\n",
      "**Paso dos: Limpiar.**  Elimine el excremento y las huellas lavando la zona con agua y una solución suave de cloro (1 parte de cloro por 10 partes de agua).  Deshágase del desorden (diarios, bolsas de papel, cartones, botellas, etc.) que proporciona refugio a las ratas. Limpie el sótano y el jardín, y guarde los artículos lejos de las paredes y elevados del suelo.  Es importante trabajar con los vecinos para una limpieza coordinada.\n",
      "\n",
      "**Paso tres: Privarlas de comida.**  Controle la basura, recoja la fruta y las nueces diariamente, no deje comida de mascotas afuera durante la noche, mantenga las palmeras y plantas de yuca podadas, y elimine o pode la hiedra y otras plantas densas lejos de techos, paredes, etc.\n",
      "\n",
      "**Paso cuatro: Cerrarles la puerta.** Selle las grietas y agujeros pequeños, tape agujeros y aberturas grandes, y cierre las madrigueras.  Controle la maleza, los matorrales y los arbustos manteniendo un espacio descubierto de 6 pulgadas alrededor de las construcciones, podando debajo de los arbustos, conservando espacio entre las plantas y evitando vegetación densa. Mantenga los jardines libres de maleza y basura.\n",
      "\n",
      "**Paso cinco: Exterminarlas.** El cebo para roedores es una forma efectiva, pero su aplicación debe ser realizada por profesionales, excepto en casas unifamiliares sin inquilinos.  Si se contrata a una empresa de control de plagas, asegúrese de que sigan las pautas de seguridad y uso del cebo.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_values = [1, 3, 5, 10]  # Diferentes valores para k\n",
    "\n",
    "for k in k_values:\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "    retriever_chain = retriever | format_docs\n",
    "    rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever_chain, # context lo obtendremos del retriever_chain\n",
    "        \"question\": RunnablePassthrough(), # question pasará directo hacia el prompt\n",
    "    }\n",
    "    | rag_prompt # prompt con las variables question y context\n",
    "    | llm # llm recibe el prompt y responde\n",
    "    | StrOutputParser() # recuperamos sólo la respuesta\n",
    ")\n",
    "    # Ejecutar la búsqueda y analizar las respuestas\n",
    "    query = \"¿Cómo controlar la población de ratas?\"\n",
    "    response = rag_chain.invoke(query)\n",
    "    print(f\"Respuesta con k={k}:\\n{response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wu_wngN5-DDy",
    "outputId": "c818eb32-ab18-4036-a549-10f6fb067f89"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta con search_type=similarity:\n",
      "Para controlar la población de ratas, es fundamental un manejo adecuado del medio ambiente, enfocándose en eliminar sus tres necesidades básicas: alimento, agua y refugio.  Esto implica prácticas de higiene tanto en el hogar como en el entorno.  Específicamente, se recomienda cosechar y recoger diariamente frutas y nueces maduras; guardar la comida sobrante de las mascotas por la noche; mantener podadas las palmeras y plantas de yuca, y eliminar o podar la hiedra y otras plantas densas lejos de estructuras; reparar fugas de agua y eliminar el agua estancada.  Adicionalmente, se debe controlar la maleza, matorrales y arbustos, manteniendo un espacio descubierto de 6 pulgadas alrededor de las construcciones, podando debajo de los arbustos, conservando espacio entre las plantas y evitando la vegetación densa.  Se debe mantener los jardines libres de maleza y basura.\n",
      "\n",
      "\n",
      "Respuesta con search_type=similarity_score_threshold:\n",
      "Para controlar la población de ratas, es fundamental un manejo adecuado del medio ambiente, enfocándose en eliminar sus tres necesidades básicas: alimento, agua y refugio.  Esto implica prácticas de higiene tanto en el hogar como en el entorno.  Específicamente, se recomienda cosechar y recoger diariamente frutas y nueces maduras; guardar la comida sobrante de las mascotas por la noche; mantener podadas las palmeras y plantas de yuca, y eliminar o podar la hiedra y otras plantas densas lejos de estructuras; reparar fugas de agua y eliminar el agua estancada.  Adicionalmente, se debe controlar la maleza, arbustos y césped alto alrededor de las construcciones, manteniendo un espacio descubierto de al menos 6 pulgadas entre la vegetación y los cimientos.  Es importante conservar espacio entre las plantas y evitar la vegetación densa para reducir la disponibilidad de refugio.\n",
      "\n",
      "\n",
      "Respuesta con search_type=mmr:\n",
      "Para controlar la población de ratas, primero debe buscar evidencia de su presencia.  Esto incluye buscar madrigueras (agujeros en la tierra o cemento de 1 a 4 pulgadas de ancho con bordes lisos, generalmente con una entrada y salida), excremento (usualmente cerca de la basura, húmedo y oscuro), agujeros y roeduras en contenedores de basura, y senderos oscuros y grasosos en paredes y césped.\n",
      "\n",
      "Una vez localizada la evidencia, para controlarlas debe quitarles todo lo que necesitan para sobrevivir.  Para ello, almacene y coloque estaciones de cebo en lugares inaccesibles para niños y mascotas.  Use únicamente productos con Número de Registro de la Agencia de Protección Ambiental (EPA) y evite productos ilegales como \"Tres Pasitos\". Reponga el cebo una vez consumido y deje las estaciones en su lugar por al menos dos semanas después de que cese toda actividad de las ratas.  Controle el área mensualmente.  Para mayor información sobre pesticidas, contacte al Centro Nacional de Información sobre Pesticidas al 1-800-858-7378.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_types = [\"similarity\", \"similarity_score_threshold\", \"mmr\"]  # Tipos de búsqueda a probar\n",
    "\n",
    "for search_type in search_types:\n",
    "\n",
    "    if search_type == \"similarity_score_threshold\":\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=search_type,\n",
    "            search_kwargs={\"k\": 3, \"score_threshold\": 0.5}  # Ajusta score_threshold entre 0 y 1\n",
    "        )\n",
    "    else:\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=search_type,\n",
    "            search_kwargs={\"k\": 3}\n",
    "        )\n",
    "    retriever_chain = retriever | format_docs\n",
    "    rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever_chain, # context lo obtendremos del retriever_chain\n",
    "        \"question\": RunnablePassthrough(), # question pasará directo hacia el prompt\n",
    "    }\n",
    "    | rag_prompt # prompt con las variables question y context\n",
    "    | llm # llm recibe el prompt y responde\n",
    "    | StrOutputParser() # recuperamos sólo la respuesta\n",
    ")\n",
    "\n",
    "    # Ejecutar la búsqueda y analizar las respuestas\n",
    "    query = \"¿Cómo controlar la población de ratas?\"\n",
    "    response = rag_chain.invoke(query)\n",
    "    print(f\"Respuesta con search_type={search_type}:\\n{response}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENJiPPM0giX8"
   },
   "source": [
    "### **2.2 Agentes (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/rcqnN2aJCSEAAAAd/secret-agent-man.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la sección anterior, en esta sección se busca habilitar **Agentes** para obtener información a través de tools y así responder la pregunta del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V47l7Mjfrk0N"
   },
   "source": [
    "#### **2.2.1 Tool de Tavily (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas al motor de búsqueda **Tavily**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R6SLKwcWr0AG",
    "outputId": "a7859054-1c8f-4bde-a452-4ef07c83e861"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To answer this question effectively, I need to find reliable information on rat control methods.  A search engine like tavily_search_results_json will be helpful.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"effective rat control methods\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://effective-rat-control.cshelpjq.com/', 'content': 'Effective Rat Control 🛑 Nov 2024. effective rat control. most effective rat control, rat control methods, rat extermination methods, effective rat control methods, home rat extermination methods, best rat control methods, most effective rodent control, do it yourself rat extermination Sure, they able to success employees, ask yourself'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: The observation provides only one URL, and the content snippet is incomplete and doesn't offer specific methods. I need a more comprehensive search to get a better answer.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"how to control rat population\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://prepperswill.com/get-rid-of-rats/', 'content': 'Controlling the rat requires control of the total local rat population and not just individuals. Winter is the best time to start a control program because breeding levels are at their lowest, and it will take the rats 12 months to return to previous population levels. The next best times are spring and fall when it takes the rats six months to'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: The second search result offers more promising information, mentioning controlling the overall population and ideal times for intervention. However, it's still a brief snippet.  I need more detailed information on methods.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"rat control methods: sanitation, trapping, poisoning\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.domyown.com/how-to-get-rid-of-rats-a-525.html', 'content': 'How to Get Rid of Rats | DIY Rat Control Guide Rats Rats Rat Bait Stations Rat Poison/Bait Rat Traps Rats Rats Products needed : selected trap, bait for trapTrapping is the preferred method of indoor control. Trapping does not require the use of rat poisons indoors. Once the rat goes into the trap, they will stand on a pedal to eat the bait that has been placed inside. While you can use rat bait poison indoors, we strongly urge you not to do so. Controlling the rat population outdoors is usually done by using rat bait poison enclosed in tamper resistant rat bait stations. Load the rat bait blocks into the stations as directed on the product label.'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: This observation provides specific methods: trapping (preferred indoors), and poisoning (using bait stations outdoors).  This is a good starting point for a comprehensive answer.  I should also consider sanitation as a preventative measure.\n",
      "\n",
      "Thought: I now know the final answer.\n",
      "\n",
      "Final Answer:  Controlar la población de ratas requiere un enfoque multifacético.  Los métodos efectivos incluyen:\n",
      "\n",
      "* **Saneamiento:** Eliminar fuentes de alimento y agua, sellar grietas y agujeros que permitan el acceso a edificios, mantener áreas limpias y libres de basura.  Esto es crucial para prevenir infestaciones.\n",
      "\n",
      "* **Trampeo:**  Es el método preferido para el control de ratas en interiores. Se utilizan trampas con cebos atractivos para capturarlas.\n",
      "\n",
      "* **Envenenamiento:**  Se utiliza principalmente en exteriores, con estaciones de cebo a prueba de manipulaciones que contienen veneno para ratas.  Es importante seguir las instrucciones del producto cuidadosamente y mantenerlo fuera del alcance de niños y mascotas.\n",
      "\n",
      "Es importante recordar que el control de plagas puede requerir la ayuda de profesionales, especialmente en casos de infestaciones severas.  El mejor momento para iniciar un programa de control es en invierno, cuando la reproducción es menor.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Controlar la población de ratas requiere un enfoque multifacético.  Los métodos efectivos incluyen:\n",
      "\n",
      "* **Saneamiento:** Eliminar fuentes de alimento y agua, sellar grietas y agujeros que permitan el acceso a edificios, mantener áreas limpias y libres de basura.  Esto es crucial para prevenir infestaciones.\n",
      "\n",
      "* **Trampeo:**  Es el método preferido para el control de ratas en interiores. Se utilizan trampas con cebos atractivos para capturarlas.\n",
      "\n",
      "* **Envenenamiento:**  Se utiliza principalmente en exteriores, con estaciones de cebo a prueba de manipulaciones que contienen veneno para ratas.  Es importante seguir las instrucciones del producto cuidadosamente y mantenerlo fuera del alcance de niños y mascotas.\n",
      "\n",
      "Es importante recordar que el control de plagas puede requerir la ayuda de profesionales, especialmente en casos de infestaciones severas.  El mejor momento para iniciar un programa de control es en invierno, cuando la reproducción es menor.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain import hub\n",
    "\n",
    "react_prompt = hub.pull(\"hwchase17/react\") # template de ReAct\n",
    "\n",
    "search = TavilySearchResults(max_results = 1) # inicializamos tool\n",
    "tools = [search]\n",
    "\n",
    "agent = create_react_agent(llm, tools, react_prompt) # primero inicializamos el agente ReAct\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) # lo transformamos a AgentExecutor para habilitar la ejecución de tools\n",
    "response = agent_executor.invoke({\"input\": \"¿Cómo controlar la población de ratas?\"})\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SonB1A-9rtRq"
   },
   "source": [
    "#### **2.2.2 Tool de Wikipedia (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas a **Wikipedia**.\n",
    "\n",
    "*Hint: Le puede ser de ayuda el siguiente [link](https://python.langchain.com/v0.1/docs/modules/tools/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bg9mDMA7FHbA",
    "outputId": "f67ce970-447c-45e8-c177-8fe595c28582"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=6ae70c0789b781ceaf064d304e1dd6e7529ef8b5be4719a1e22ad7d03e58e3e7\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "ehJJpoqsr26-",
    "outputId": "260c864c-fa66-4cdb-8f21-a3e844d3bb14"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-d3dd6515f135>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Inicializar el agente ReAct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_react_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreact_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Convertir el agente a un AgentExecutor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/react/agent.py\u001b[0m in \u001b[0;36mcreate_react_agent\u001b[0;34m(llm, tools, prompt, output_parser, tools_renderer, stop_sequence)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     prompt = prompt.partial(\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtools_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mtool_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/tools/render.py\u001b[0m in \u001b[0;36mrender_text_description\u001b[0;34m(tools)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{tool.name}{sig} - {tool.description}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{tool.name} - {tool.description}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mdescriptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=100)\n",
    "tool = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "\n",
    "# Inicializar el agente ReAct\n",
    "agent = create_react_agent(llm, tool, react_prompt)\n",
    "\n",
    "# Convertir el agente a un AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=[tool], verbose=True)\n",
    "\n",
    "# Ejecutar la consulta\n",
    "response = agent_executor.run({\"input\": \"¿Cómo controlar la población de ratas?\"})\n",
    "\n",
    "# Imprimir la respuesta\n",
    "print(response[\"output\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvUIMdX6r0ne"
   },
   "source": [
    "#### **2.2.3 Crear Agente (0.3 puntos)**\n",
    "\n",
    "Crear un agente que pueda responder preguntas preguntas usando las *tools* antes generadas. Asegúrese que su agente responda en español. Por último, guarde el agente en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pD1_n0wrsDI5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKV0JxK3r-XG"
   },
   "source": [
    "#### **2.2.4 Verificación de respuestas (0.3 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente y asegúrese que el agente esté ocupando correctamente las tools disponibles. ¿En qué casos el agente debería ocupar la tool de Tavily? ¿En qué casos debería ocupar la tool de Wikipedia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pqo2dsxvywW_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZbDTYiogquv"
   },
   "source": [
    "### **2.3 Multi Agente (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/r7QMJLxU4BoAAAAd/this-is-getting-out-of-hand-star-wars.gif\"\n",
    "\" width=\"450\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsección es encapsular las funcionalidades creadas en una solución multiagente con un **supervisor**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-iUfH0WvI6m"
   },
   "source": [
    "#### **2.3.1 Generando Tools (0.5 puntos)**\n",
    "\n",
    "Transforme la solución RAG de la sección 2.1 y el agente de la sección 2.2 a *tools* (una tool por cada uno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pw1cfTtvv1AZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQYNjT_0vPCg"
   },
   "source": [
    "#### **2.3.2 Agente Supervisor (0.5 puntos)**\n",
    "\n",
    "Habilite un agente que tenga acceso a las tools del punto anterior y pueda responder preguntas relacionadas. Almacene este agente en una variable llamada supervisor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yv2ZY0BAv1RD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea3zWlvyvY7K"
   },
   "source": [
    "#### **2.3.3 Verificación de respuestas (0.25 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente repitiendo las preguntas realizadas en las secciones 2.1.4 y 2.2.4 y comente sus resultados. ¿Cómo varían las respuestas bajo este enfoque?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_1t0zkgv1qW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qb8bdAmYvgwn"
   },
   "source": [
    "#### **2.3.4 Análisis (0.25 puntos)**\n",
    "\n",
    "¿Qué diferencias tiene este enfoque con la solución *Router* vista en clases? Nombre al menos una ventaja y desventaja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAUlJxqoLK5r"
   },
   "source": [
    "`escriba su respuesta acá`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JWVSuWiZ8Mj"
   },
   "source": [
    "### **2.4 Memoria (Bonus +0.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/Gs95aiElrscAAAAd/memory-unlocked-ratatouille-critic.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una de las principales falencias de las soluciones que hemos visto hasta ahora es que nuestro chat no responde las interacciones anteriores, por ejemplo:\n",
    "\n",
    "- Pregunta 1: \"Hola! mi nombre es Sebastián\"\n",
    "  - Respuesta esperada: \"Hola Sebastián! ...\"\n",
    "- Pregunta 2: \"Cual es mi nombre?\"\n",
    "  - Respuesta actual: \"Lo siento pero no conozco tu nombre :(\"\n",
    "  - **Respuesta esperada: \"Tu nombre es Sebastián\"**\n",
    "\n",
    "Para solucionar esto, se les solicita agregar un componente de **memoria** a la solución entregada en el punto 2.3.\n",
    "\n",
    "**Nota: El Bonus es válido <u>sólo para la sección 2 de Large Language Models.</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6Y7tIPJLPfB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFc3jBT5g0kT"
   },
   "source": [
    "### **2.5 Despliegue (0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/IytHqOp52EsAAAAd/you-get-a-deploy-deploy.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una vez tengan los puntos anteriores finalizados, toca la etapa de dar a conocer lo que hicimos! Para eso, vamos a desplegar nuestro modelo a través de `gradio`, una librería especializada en el levantamiento rápido de demos basadas en ML.\n",
    "\n",
    "Primero instalamos la librería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8TsvnCPbkIA"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJBztEUovKsF"
   },
   "source": [
    "Luego sólo deben ejecutar el siguiente código e interactuar con la interfaz a través del notebook o del link generado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3KedQSvg1-n"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def agent_response(message, history):\n",
    "  '''\n",
    "  Función para gradio, recibe mensaje e historial, devuelte la respuesta del chatbot.\n",
    "  '''\n",
    "  # get chatbot response\n",
    "  response = ... # rellenar con la respuesta de su chat\n",
    "\n",
    "  # assert\n",
    "  assert type(response) == str, \"output de route_question debe ser string\"\n",
    "\n",
    "  # \"streaming\" response\n",
    "  for i in range(len(response)):\n",
    "    time.sleep(0.015)\n",
    "    yield response[: i+1]\n",
    "\n",
    "gr.ChatInterface(\n",
    "    agent_response,\n",
    "    type=\"messages\",\n",
    "    title=\"Chatbot MDS7202\", # Pueden cambiar esto si lo desean\n",
    "    description=\"Hola! Soy un chatbot muy útil :)\", # también la descripción\n",
    "    theme=\"soft\",\n",
    "    ).launch(\n",
    "        share=True, # pueden compartir el link a sus amig@s para que interactuen con su chat!\n",
    "        debug = False,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
