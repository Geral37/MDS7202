{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyPTffTLug7i"
   },
   "source": [
    "# **Laboratorio 11: LLM y Agentes Aut贸nomos **\n",
    "\n",
    "MDS7202: Laboratorio de Programaci贸n Cient铆fica para Ciencia de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pbWVyntzbvL"
   },
   "source": [
    "### **Cuerpo Docente:**\n",
    "\n",
    "- Profesores: Ignacio Meza, Sebasti谩n Tinoco\n",
    "- Auxiliar: Eduardo Moya\n",
    "- Ayudantes: Nicol谩s Ojeda, Melanie Pe帽a, Valentina Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy6ikgVYzghB"
   },
   "source": [
    "### Equipo: Ratas.py \n",
    "\n",
    "- Nombre de alumno 1: Geraldyn P茅rez\n",
    "- Nombre de alumno 2: Diego Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMJ-owchzjFf"
   },
   "source": [
    "### **Link de repositorio de GitHub:** [Repositorio](https://github.com/Geral37/MDS7202.git)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUuwsXrKzmkK"
   },
   "source": [
    "## **Temas a tratar**\n",
    "\n",
    "- Reinforcement Learning\n",
    "- Large Language Models\n",
    "\n",
    "## **Reglas:**\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser谩n respondidos por este medio.\n",
    "- Prohibidas las copias.\n",
    "- Pueden usar cualquer matrial del curso que estimen conveniente.\n",
    "\n",
    "### **Objetivos principales del laboratorio**\n",
    "\n",
    "- Resoluci贸n de problemas secuenciales usando Reinforcement Learning\n",
    "- Habilitar un Chatbot para entregar respuestas 煤tiles usando Large Language Models.\n",
    "\n",
    "El laboratorio deber谩 ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m谩ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m谩s eficientes que los iteradores nativos sobre DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hmHHQ9BuyAG"
   },
   "source": [
    "## **1. Reinforcement Learning (2.0 puntos)**\n",
    "\n",
    "En esta secci贸n van a usar m茅todos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gOcejYb6uzOO",
    "outputId": "7c977d9e-6aa9-4117-c11c-d992477100ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/958.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m\u001b[0m\u001b[91m\u001b[0m \u001b[32m952.3/958.1 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -qqq gymnasium stable_baselines3\n",
    "!pip install -qqq swig\n",
    "!pip install -qqq gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBPet_Mq8dX9"
   },
   "source": [
    "### **1.1 Blackjack (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "La idea de esta subsecci贸n es que puedan implementar m茅todos de RL y as铆 generar una estrategia para jugar el cl谩sico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
    "\n",
    "Comencemos primero preparando el ambiente. El siguiente bloque de c贸digo transforma las observaciones del ambiente a `np.array`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpZ8bBKk9ZlU"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import MultiDiscrete\n",
    "import numpy as np\n",
    "\n",
    "class FlattenObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(FlattenObservation, self).__init__(env)\n",
    "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.array(observation).flatten()\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "env = FlattenObservation(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ6J1_-Y9nHO"
   },
   "source": [
    "#### **1.1.1 Descripci贸n de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripci贸n sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulaci贸n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5i1Wt1p770x"
   },
   "source": [
    "`escriba su respuesta ac谩`\n",
    "\n",
    "El entorno Blackjack es un juego de cartas modelado como un problema de decisi贸n secuencial, donde el objetivo es maximizar la recompensa jugando contra un crupier. El jugador debe tomar decisiones en cada estado bas谩ndose en su mano, la carta visible del crupier y la posibilidad de usar un as como 11. Este entorno se considera un Problema de Decisi贸n de Markov (MDP) porque satisface la propiedad de memoria de Markov: el siguiente estado y recompensa dependen 煤nicamente del estado actual y la acci贸n tomada.\n",
    "\n",
    "El espacio de acci贸n est谩 representado como un rango de valores discretos, {0, 1}, que indican las dos decisiones posibles del jugador. La acci贸n 0 corresponde a plantarse (\"Palo\"), terminando el turno del jugador y permitiendo que el crupier act煤e. Por otro lado, la acci贸n 1 representa pedir una carta adicional (\"Golpe\"), lo que incrementa la suma de las cartas del jugador, con el riesgo de superar 21.\n",
    "\n",
    "El espacio de observaci贸n est谩 definido como una tupla de tres elementos: la suma actual de las cartas del jugador, el valor de la carta visible del crupier (un n煤mero entre 1 y 10, donde 1 representa un as) y un indicador binario que se帽ala si el jugador tiene un as utilizable (es decir, si el as puede contar como 11 sin exceder 21). Este dise帽o permite al jugador tomar decisiones estrat茅gicas basadas en el estado del juego.\n",
    "\n",
    "Las recompensas en este entorno reflejan los resultados del juego: el jugador recibe +1 si gana, -1 si pierde y 0 en caso de empate. Si el jugador gana con un blackjack natural (un as y una carta de valor 10 como mano inicial), puede recibir una recompensa adicional de +1.5 si esta regla est谩 habilitada. De lo contrario, la recompensa es de +1.\n",
    "\n",
    "Un episodio termina cuando el jugador elige pedir carta y su mano supera 21, o cuando decide plantarse, momento en el cual el crupier juega y se determina el resultado final. Cabe destacar que un as siempre se considerar谩 utilizable (como 11) a menos que hacerlo cause que el jugador supere 21. Esto asegura que el juego se ajuste a las reglas del blackjack est谩ndar y facilite la implementaci贸n del entorno como un MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmcX6bRC9agQ"
   },
   "source": [
    "#### **1.1.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci贸n 5000 veces y reporte el promedio y desviaci贸n de las recompensas. 驴C贸mo calificar铆a el performance de esta pol铆tica? 驴C贸mo podr铆a interpretar las recompensas obtenidas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9p2PrLLR9yju",
    "outputId": "c6e0a781-2032-46a9-8240-f598489ba9b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensas: -0.3746\n",
      "Desviaci贸n est谩ndar de las recompensas: 0.9048065207545755\n"
     ]
    }
   ],
   "source": [
    "total_rewards = [] # recompensas\n",
    "\n",
    "# Repetir la simulaci贸n 5000 veces\n",
    "for _ in range(5000):\n",
    "    # Reiniciar el entorno\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Elegir una acci贸n aleatoria (el espacio de acci贸n tiene 2 dimensiones: \"acci贸n 0\" o \"acci贸n 1\")\n",
    "        action = env.action_space.sample()  # Esto selecciona una acci贸n aleatoria\n",
    "\n",
    "        # Tomar la acci贸n en el entorno\n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # Acumular la recompensa\n",
    "        total_reward += reward\n",
    "\n",
    "    # Guardar la recompensa total del episodio\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "# Convertir la lista de recompensas a un array de numpy para an谩lisis\n",
    "total_rewards = np.array(total_rewards)\n",
    "\n",
    "# Calcular el promedio y desviaci贸n est谩ndar\n",
    "average_reward = np.mean(total_rewards)\n",
    "std_deviation = np.std(total_rewards)\n",
    "\n",
    "# Reportar los resultados\n",
    "print(f\"Promedio de recompensas: {average_reward}\")\n",
    "print(f\"Desviaci贸n est谩ndar de las recompensas: {std_deviation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEF8x4dLZVeR"
   },
   "source": [
    "Es mala perfomance porque la estrategia siempre entrega p茅rdida en valor esperado. El juego aleatorio estar铆a cargado en contra del jugador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEO_dY4x_SJu"
   },
   "source": [
    "#### **1.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9JsFA1wGmnH",
    "outputId": "e1a63bb5-6a7f-4940-f6de-09cc37fa2d1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.14     |\n",
      "|    ep_rew_mean        | -0.27    |\n",
      "| time/                 |          |\n",
      "|    fps                | 154      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.287   |\n",
      "|    explained_variance | 0.409    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.589    |\n",
      "|    value_loss         | 0.423    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03     |\n",
      "|    ep_rew_mean        | -0.15    |\n",
      "| time/                 |          |\n",
      "|    fps                | 207      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.182   |\n",
      "|    explained_variance | 0.777    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.00224  |\n",
      "|    value_loss         | 0.215    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.05      |\n",
      "|    ep_rew_mean        | -0.27     |\n",
      "| time/                 |           |\n",
      "|    fps                | 212       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.147    |\n",
      "|    explained_variance | 0.163     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -0.000519 |\n",
      "|    value_loss         | 0.804     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1        |\n",
      "|    ep_rew_mean        | -0.12    |\n",
      "| time/                 |          |\n",
      "|    fps                | 224      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0654  |\n",
      "|    explained_variance | 0.122    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.00071 |\n",
      "|    value_loss         | 0.843    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02     |\n",
      "|    ep_rew_mean        | -0.03    |\n",
      "| time/                 |          |\n",
      "|    fps                | 242      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0319  |\n",
      "|    explained_variance | 0.37     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.00198  |\n",
      "|    value_loss         | 0.681    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01     |\n",
      "|    ep_rew_mean        | -0.33    |\n",
      "| time/                 |          |\n",
      "|    fps                | 257      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0877  |\n",
      "|    explained_variance | -0.199   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.0172   |\n",
      "|    value_loss         | 1.49     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01     |\n",
      "|    ep_rew_mean        | -0.02    |\n",
      "| time/                 |          |\n",
      "|    fps                | 268      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0326  |\n",
      "|    explained_variance | 0.861    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.00223 |\n",
      "|    value_loss         | 0.232    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1        |\n",
      "|    ep_rew_mean        | -0.11    |\n",
      "| time/                 |          |\n",
      "|    fps                | 276      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0422  |\n",
      "|    explained_variance | -0.187   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.00325 |\n",
      "|    value_loss         | 0.95     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1        |\n",
      "|    ep_rew_mean        | -0.14    |\n",
      "| time/                 |          |\n",
      "|    fps                | 285      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.039   |\n",
      "|    explained_variance | 0.0699   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.00129  |\n",
      "|    value_loss         | 1.41     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01     |\n",
      "|    ep_rew_mean        | -0.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 291      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 17       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0298  |\n",
      "|    explained_variance | -0.0323  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -0.00329 |\n",
      "|    value_loss         | 0.923    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1        |\n",
      "|    ep_rew_mean        | -0.22    |\n",
      "| time/                 |          |\n",
      "|    fps                | 289      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0175  |\n",
      "|    explained_variance | 0.341    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.00211  |\n",
      "|    value_loss         | 0.838    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1        |\n",
      "|    ep_rew_mean        | -0.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 290      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0344  |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.00359 |\n",
      "|    value_loss         | 0.283    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01     |\n",
      "|    ep_rew_mean        | -0.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 296      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0277  |\n",
      "|    explained_variance | -0.331   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -0.00235 |\n",
      "|    value_loss         | 0.865    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.01      |\n",
      "|    ep_rew_mean        | -0.2      |\n",
      "| time/                 |           |\n",
      "|    fps                | 300       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0331   |\n",
      "|    explained_variance | 0.302     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -4.47e-05 |\n",
      "|    value_loss         | 0.674     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04     |\n",
      "|    ep_rew_mean        | -0.04    |\n",
      "| time/                 |          |\n",
      "|    fps                | 303      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 24       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0341  |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.00154 |\n",
      "|    value_loss         | 0.19     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02     |\n",
      "|    ep_rew_mean        | -0.14    |\n",
      "| time/                 |          |\n",
      "|    fps                | 306      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 26       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0968  |\n",
      "|    explained_variance | -0.0074  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.0495   |\n",
      "|    value_loss         | 1.19     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.03      |\n",
      "|    ep_rew_mean        | -0.29     |\n",
      "| time/                 |           |\n",
      "|    fps                | 310       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0247   |\n",
      "|    explained_variance | 0.0856    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -0.000534 |\n",
      "|    value_loss         | 0.588     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1        |\n",
      "|    ep_rew_mean        | -0.09    |\n",
      "| time/                 |          |\n",
      "|    fps                | 313      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 28       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0165  |\n",
      "|    explained_variance | 0.0529   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.000439 |\n",
      "|    value_loss         | 0.958    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03     |\n",
      "|    ep_rew_mean        | -0.13    |\n",
      "| time/                 |          |\n",
      "|    fps                | 314      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 30       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0206  |\n",
      "|    explained_variance | 0.454    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.000165 |\n",
      "|    value_loss         | 0.583    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04     |\n",
      "|    ep_rew_mean        | -0.14    |\n",
      "| time/                 |          |\n",
      "|    fps                | 311      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 32       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.316   |\n",
      "|    explained_variance | 0.537    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -0.063   |\n",
      "|    value_loss         | 0.445    |\n",
      "------------------------------------\n",
      "Episodio 1 - Recompensa Total: 1.0\n",
      "Episodio 2 - Recompensa Total: -1.0\n",
      "Episodio 3 - Recompensa Total: -1.0\n",
      "Episodio 4 - Recompensa Total: -1.0\n",
      "Episodio 5 - Recompensa Total: -1.0\n",
      "Episodio 6 - Recompensa Total: 1.0\n",
      "Episodio 7 - Recompensa Total: -1.0\n",
      "Episodio 8 - Recompensa Total: -1.0\n",
      "Episodio 9 - Recompensa Total: -1.0\n",
      "Episodio 10 - Recompensa Total: 1.0\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import A2C\n",
    "\n",
    "# Crear el modelo A2C con una red neuronal de 64 neuronas en una capa oculta\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Entrenar el modelo (10000 pasos de entrenamiento)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Evaluar el modelo en 10 episodios\n",
    "for episode in range(10):\n",
    "    obs, info = env.reset()  # Reiniciar el entorno\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        # Elegir una acci贸n usando el modelo entrenado\n",
    "        action, _states = model.predict(obs)\n",
    "\n",
    "        # Ejecutar la acci贸n en el entorno\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # Acumular la recompensa\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episodio {episode + 1} - Recompensa Total: {total_reward}\")\n",
    "\n",
    "# Guardar el modelo entrenado (opcional)\n",
    "model.save(\"a2c_blackjack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-bpdb8wZID1"
   },
   "source": [
    "#### **1.1.4 Evaluaci贸n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.1.2 pero utilizando el modelo entrenado. 驴C贸mo es el performance de su agente? 驴Es mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-d7d8GFf7F6",
    "outputId": "b80c043b-0d5a-4a9d-8a42-a35eb64a618e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensas: -0.1726\n",
      "Desviaci贸n est谩ndar de las recompensas: 0.957710415522354\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo A2C entrenado (si lo tienes guardado)\n",
    "#model = A2C.load(\"a2c_blackjack\")\n",
    "\n",
    "# Lista para almacenar las recompensas\n",
    "total_rewards = []\n",
    "\n",
    "# Repetir la simulaci贸n 5000 veces\n",
    "for _ in range(5000):\n",
    "    # Reiniciar el entorno\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Elegir una acci贸n usando el modelo entrenado\n",
    "        action, _states = model.predict(observation)\n",
    "\n",
    "        # Tomar la acci贸n en el entorno\n",
    "        observation, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "        # Acumular la recompensa\n",
    "        total_reward += reward\n",
    "\n",
    "    # Guardar la recompensa total del episodio\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "# Convertir la lista de recompensas a un array de numpy para an谩lisis\n",
    "total_rewards = np.array(total_rewards)\n",
    "\n",
    "# Calcular el promedio y desviaci贸n est谩ndar\n",
    "average_reward = np.mean(total_rewards)\n",
    "std_deviation = np.std(total_rewards)\n",
    "\n",
    "# Reportar los resultados\n",
    "print(f\"Promedio de recompensas: {average_reward}\")\n",
    "print(f\"Desviaci贸n est谩ndar de las recompensas: {std_deviation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iqg4ygJkbkHY"
   },
   "source": [
    "Mantiene baja performance pero mejora respecto al baseline. Se obtiene un promedio de p茅rdidas menor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO-EsAaPAYEm"
   },
   "source": [
    "#### **1.1.5 Estudio de acciones (0.2 puntos)**\n",
    "\n",
    "Genere una funci贸n que reciba un estado y retorne la accion del agente. Luego, use esta funci贸n para entregar la acci贸n escogida frente a los siguientes escenarios:\n",
    "\n",
    "- Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
    "- Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
    "\n",
    "驴Son coherentes sus acciones con las reglas del juego?\n",
    "\n",
    "Hint: 驴A que clase de python pertenecen los estados? Pruebe a usar el m茅todo `.reset` para saberlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8BnvgONQcTC7",
    "outputId": "abb9f4fe-2d68-42da-981d-147b3e6265cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([15, 10,  1]), {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fh8XlGyzwtRp",
    "outputId": "f371968e-ae3f-4e6a-c8c7-3830a80ef29f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acci贸n en el escenario 1 (Agente suma 6, Dealer muestra 7, sin as): 0\n",
      "Acci贸n en el escenario 2 (Agente suma 19, Dealer muestra 3, con as): 0\n"
     ]
    }
   ],
   "source": [
    "# Funci贸n para obtener la acci贸n del agente dado un estado\n",
    "def obtener_accion(state):\n",
    "    # Usamos el modelo entrenado para predecir la acci贸n en base al estado\n",
    "    action, _states = model.predict(state)\n",
    "    return action\n",
    "\n",
    "# Primer escenario: Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene un as\n",
    "# Para simular este escenario, creamos un estado artificialmente. Suponemos que el estado es un vector de numpy.\n",
    "# Nota: La representaci贸n del estado depender谩 de c贸mo est茅 estructurado el entorno. Para fines de este ejercicio,\n",
    "# asumimos que el estado es un array que contiene [suma_agente, suma_dealer, tiene_as].\n",
    "# Este es un ejemplo simplificado.\n",
    "\n",
    "estado_escenario_1 = np.array([6, 7, 0])  # Suma del agente es 6, dealer muestra 7, agente no tiene un as\n",
    "accion_1 = obtener_accion(estado_escenario_1)\n",
    "\n",
    "# Segundo escenario: Suma de cartas del agente es 19, dealer muestra un 3, agente tiene un as\n",
    "estado_escenario_2 = np.array([19, 3, 1])  # Suma del agente es 19, dealer muestra 3, agente tiene un as\n",
    "accion_2 = obtener_accion(estado_escenario_2)\n",
    "\n",
    "# Imprimir las acciones en ambos escenarios\n",
    "print(f\"Acci贸n en el escenario 1 (Agente suma 6, Dealer muestra 7, sin as): {accion_1}\")\n",
    "print(f\"Acci贸n en el escenario 2 (Agente suma 19, Dealer muestra 3, con as): {accion_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEqCTqqroh03"
   },
   "source": [
    "### **1.2 LunarLander**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.redd.it/097t6tk29zf51.jpg\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la secci贸n 2.1, en esta secci贸n usted se encargar谩 de implementar una gente de RL que pueda resolver el ambiente `LunarLander`.\n",
    "\n",
    "Comencemos preparando el ambiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nvQUyuZ_FtZ4",
    "outputId": "603d8263-e93d-4ee6-9550-94aa2b5b3a36"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\", continuous = True) # notar el par谩metro continuous = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBU4lGX3wpN6"
   },
   "source": [
    "Noten que se especifica el par谩metro `continuous = True`. 驴Que implicancias tiene esto sobre el ambiente?\n",
    "\n",
    "Adem谩s, se le facilita la funci贸n `export_gif` para el ejercicio 2.2.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRiWpSo9yfr9"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "def export_gif(model, n = 5):\n",
    "  '''\n",
    "  funci贸n que exporta a gif el comportamiento del agente en n episodios\n",
    "  '''\n",
    "  images = []\n",
    "  for episode in range(n):\n",
    "    obs = model.env.reset()\n",
    "    img = model.env.render()\n",
    "    done = False\n",
    "    while not done:\n",
    "      images.append(img)\n",
    "      action, _ = model.predict(obs)\n",
    "      obs, reward, done, info = model.env.step(action)\n",
    "      img = model.env.render(mode=\"rgb_array\")\n",
    "\n",
    "  imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk5VJVppXh3N"
   },
   "source": [
    "#### **1.2.1 Descripci贸n de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripci贸n sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulaci贸n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas. 驴Como se distinguen las acciones de este ambiente en comparaci贸n a `Blackjack`?\n",
    "\n",
    "Nota: recuerde que se especific贸 el par谩metro `continuous = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb-u9LUE8O9a"
   },
   "source": [
    "`escriba su respuesta ac谩`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YChodtNQwzG2"
   },
   "source": [
    "#### **1.2.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci贸n 10 veces y reporte el promedio y desviaci贸n de las recompensas. 驴C贸mo calificar铆a el performance de esta pol铆tica?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bwc3A0GX7a8",
    "outputId": "641020ad-b365-4181-f3ab-2fbe7912d3d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensas: -210.40957651492937\n",
      "Desviaci贸n est谩ndar de las recompensas: 156.8861580778027\n"
     ]
    }
   ],
   "source": [
    "# Lista para almacenar las recompensas totales de cada episodio\n",
    "rewards = []\n",
    "\n",
    "# Ejecutar la simulaci贸n 10 veces con acciones aleatorias\n",
    "for episode in range(10):\n",
    "    obs, info = env.reset()  # Reiniciar el entorno\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Seleccionar una acci贸n aleatoria\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Tomar la acci贸n en el entorno\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # Acumular la recompensa total\n",
    "        total_reward += reward\n",
    "\n",
    "    # Guardar la recompensa total del episodio\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# Convertir la lista de recompensas a un array de numpy para an谩lisis\n",
    "rewards = np.array(rewards)\n",
    "\n",
    "# Calcular el promedio y desviaci贸n est谩ndar de las recompensas\n",
    "average_reward = np.mean(rewards)\n",
    "std_deviation = np.std(rewards)\n",
    "\n",
    "# Reportar los resultados\n",
    "print(f\"Promedio de recompensas: {average_reward}\")\n",
    "print(f\"Desviaci贸n est谩ndar de las recompensas: {std_deviation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDENs-lRc4l7"
   },
   "source": [
    "Al igual que el baseline del punto anterior, en promedio se obtiene p茅rdidas al seguir esta estrategia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQrZVQflX_5f"
   },
   "source": [
    "#### **1.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_6Ia9uoF7Hs",
    "outputId": "cf3137df-bfc1-4ea0-bf5e-a0ede179995c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.2     |\n",
      "|    ep_rew_mean     | -721     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 373      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.07     |\n",
      "|    critic_loss     | 69       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 272      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93       |\n",
      "|    ep_rew_mean     | -937     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 744      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.6     |\n",
      "|    critic_loss     | 35.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 643      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.7     |\n",
      "|    ep_rew_mean     | -976     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1088     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 32.3     |\n",
      "|    critic_loss     | 7.81     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 987      |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 90.8      |\n",
      "|    ep_rew_mean     | -1.03e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 158       |\n",
      "|    time_elapsed    | 9         |\n",
      "|    total_timesteps | 1452      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 41.8      |\n",
      "|    critic_loss     | 10.1      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 1351      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 91.8      |\n",
      "|    ep_rew_mean     | -1.08e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 159       |\n",
      "|    time_elapsed    | 11        |\n",
      "|    total_timesteps | 1837      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 55.3      |\n",
      "|    critic_loss     | 18.7      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 1736      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 93.2      |\n",
      "|    ep_rew_mean     | -1.11e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 159       |\n",
      "|    time_elapsed    | 13        |\n",
      "|    total_timesteps | 2237      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 59.5      |\n",
      "|    critic_loss     | 106       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 2136      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 91.2      |\n",
      "|    ep_rew_mean     | -1.11e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 153       |\n",
      "|    time_elapsed    | 16        |\n",
      "|    total_timesteps | 2553      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 69.6      |\n",
      "|    critic_loss     | 53.4      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 2452      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 91        |\n",
      "|    ep_rew_mean     | -1.13e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 32        |\n",
      "|    fps             | 153       |\n",
      "|    time_elapsed    | 18        |\n",
      "|    total_timesteps | 2911      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 82.2      |\n",
      "|    critic_loss     | 87        |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 2810      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 91.2      |\n",
      "|    ep_rew_mean     | -1.14e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 36        |\n",
      "|    fps             | 154       |\n",
      "|    time_elapsed    | 21        |\n",
      "|    total_timesteps | 3285      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 87.2      |\n",
      "|    critic_loss     | 84.7      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 3184      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 90.9      |\n",
      "|    ep_rew_mean     | -1.13e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 40        |\n",
      "|    fps             | 155       |\n",
      "|    time_elapsed    | 23        |\n",
      "|    total_timesteps | 3636      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 94        |\n",
      "|    critic_loss     | 211       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 3535      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 90.5      |\n",
      "|    ep_rew_mean     | -1.13e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 44        |\n",
      "|    fps             | 156       |\n",
      "|    time_elapsed    | 25        |\n",
      "|    total_timesteps | 3980      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 103       |\n",
      "|    critic_loss     | 119       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 3879      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 89.5      |\n",
      "|    ep_rew_mean     | -1.11e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 48        |\n",
      "|    fps             | 155       |\n",
      "|    time_elapsed    | 27        |\n",
      "|    total_timesteps | 4295      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 102       |\n",
      "|    critic_loss     | 115       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 4194      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 89.2      |\n",
      "|    ep_rew_mean     | -1.11e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 52        |\n",
      "|    fps             | 152       |\n",
      "|    time_elapsed    | 30        |\n",
      "|    total_timesteps | 4637      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 111       |\n",
      "|    critic_loss     | 271       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 4536      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.4     |\n",
      "|    ep_rew_mean     | -1.1e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 4949     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 113      |\n",
      "|    critic_loss     | 258      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4848     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.5     |\n",
      "|    ep_rew_mean     | -1.1e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 5313     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 118      |\n",
      "|    critic_loss     | 183      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5212     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.4     |\n",
      "|    ep_rew_mean     | -1.1e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 5660     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 113      |\n",
      "|    critic_loss     | 326      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5559     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 87.6      |\n",
      "|    ep_rew_mean     | -1.09e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 68        |\n",
      "|    fps             | 154       |\n",
      "|    time_elapsed    | 38        |\n",
      "|    total_timesteps | 5955      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 122       |\n",
      "|    critic_loss     | 217       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 5854      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 87.6      |\n",
      "|    ep_rew_mean     | -1.09e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 72        |\n",
      "|    fps             | 154       |\n",
      "|    time_elapsed    | 40        |\n",
      "|    total_timesteps | 6308      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 129       |\n",
      "|    critic_loss     | 317       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 6207      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 87.2      |\n",
      "|    ep_rew_mean     | -1.08e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 76        |\n",
      "|    fps             | 152       |\n",
      "|    time_elapsed    | 43        |\n",
      "|    total_timesteps | 6629      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 138       |\n",
      "|    critic_loss     | 247       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 6528      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 87.5      |\n",
      "|    ep_rew_mean     | -1.09e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 80        |\n",
      "|    fps             | 153       |\n",
      "|    time_elapsed    | 45        |\n",
      "|    total_timesteps | 6998      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -10.4     |\n",
      "|    critic_loss     | 234       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 6897      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 86.5      |\n",
      "|    ep_rew_mean     | -1.05e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 84        |\n",
      "|    fps             | 153       |\n",
      "|    time_elapsed    | 47        |\n",
      "|    total_timesteps | 7270      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.61     |\n",
      "|    critic_loss     | 497       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 7169      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 86.3      |\n",
      "|    ep_rew_mean     | -1.02e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 88        |\n",
      "|    fps             | 154       |\n",
      "|    time_elapsed    | 49        |\n",
      "|    total_timesteps | 7597      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02     |\n",
      "|    critic_loss     | 677       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 7496      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.9     |\n",
      "|    ep_rew_mean     | -999     |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 7997     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.63     |\n",
      "|    critic_loss     | 163      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7896     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.3     |\n",
      "|    ep_rew_mean     | -974     |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 8475     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.8     |\n",
      "|    critic_loss     | 157      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8374     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.8     |\n",
      "|    ep_rew_mean     | -944     |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 8882     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 28.3     |\n",
      "|    critic_loss     | 125      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8781     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89.6     |\n",
      "|    ep_rew_mean     | -936     |\n",
      "| time/              |          |\n",
      "|    episodes        | 104      |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 61       |\n",
      "|    total_timesteps | 9334     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 29       |\n",
      "|    critic_loss     | 48.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9233     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.8     |\n",
      "|    ep_rew_mean     | -904     |\n",
      "| time/              |          |\n",
      "|    episodes        | 108      |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 64       |\n",
      "|    total_timesteps | 9824     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 36.4     |\n",
      "|    critic_loss     | 34.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9723     |\n",
      "---------------------------------\n",
      "Episodio 1 - Recompensa Total: -524.0027137816859\n",
      "Episodio 2 - Recompensa Total: -572.4100436709303\n",
      "Episodio 3 - Recompensa Total: -430.4101743437193\n",
      "Episodio 4 - Recompensa Total: -55.04549564251315\n",
      "Episodio 5 - Recompensa Total: -230.24836993726544\n",
      "Episodio 6 - Recompensa Total: -440.8368314316917\n",
      "Episodio 7 - Recompensa Total: -88.98140390626085\n",
      "Episodio 8 - Recompensa Total: -500.9831978302545\n",
      "Episodio 9 - Recompensa Total: -492.1418546399306\n",
      "Episodio 10 - Recompensa Total: -243.72989140572906\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import TD3\n",
    "\n",
    "# Crear el modelo A2C con una red neuronal de 64 neuronas en una capa oculta\n",
    "model2 = TD3(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Entrenar el modelo (10000 pasos de entrenamiento)\n",
    "model2.learn(total_timesteps=10000)\n",
    "\n",
    "# Evaluar el modelo en 10 episodios\n",
    "for episode in range(10):\n",
    "    obs, info = env.reset()  # Reiniciar el entorno\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        # Elegir una acci贸n usando el modelo entrenado\n",
    "        action, _states = model2.predict(obs)\n",
    "\n",
    "        # Ejecutar la acci贸n en el entorno\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # Acumular la recompensa\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episodio {episode + 1} - Recompensa Total: {total_reward}\")\n",
    "\n",
    "# Guardar el modelo entrenado (opcional)\n",
    "model2.save(\"td2_LunarLander\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z-oIUSrlAsY"
   },
   "source": [
    "#### **1.2.4 Evaluaci贸n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.2.2 pero utilizando el modelo entrenado. 驴C贸mo es el performance de su agente? 驴Es mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ophyU3KrWrwl",
    "outputId": "98009c8f-d810-4522-c546-eebf462b2542"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensas: -286.66797663417617\n",
      "Desviaci贸n est谩ndar de las recompensas: 139.41690965104962\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo A2C entrenado (si lo tienes guardado)\n",
    "#model2 = TD3.load(\"td2_LunarLander\")\n",
    "\n",
    "# Lista para almacenar las recompensas\n",
    "total_rewards = []\n",
    "\n",
    "# Repetir la simulaci贸n 5000 veces\n",
    "for _ in range(5000):\n",
    "    # Reiniciar el entorno\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Elegir una acci贸n usando el modelo entrenado\n",
    "        action, _states = model2.predict(observation)\n",
    "\n",
    "        # Tomar la acci贸n en el entorno\n",
    "        observation, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "        # Acumular la recompensa\n",
    "        total_reward += reward\n",
    "\n",
    "    # Guardar la recompensa total del episodio\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "# Convertir la lista de recompensas a un array de numpy para an谩lisis\n",
    "total_rewards = np.array(total_rewards)\n",
    "\n",
    "# Calcular el promedio y desviaci贸n est谩ndar\n",
    "average_reward = np.mean(total_rewards)\n",
    "std_deviation = np.std(total_rewards)\n",
    "\n",
    "# Reportar los resultados\n",
    "print(f\"Promedio de recompensas: {average_reward}\")\n",
    "print(f\"Desviaci贸n est谩ndar de las recompensas: {std_deviation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6Xw4YHT3P5d"
   },
   "source": [
    "#### **1.2.5 Optimizaci贸n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita los ejercicios 1.2.3 y 1.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente par谩metros como:\n",
    "- `total_timesteps`\n",
    "- `learning_rate`\n",
    "- `batch_size`\n",
    "\n",
    "Una vez optimizado el modelo, use la funci贸n `export_gif` para estudiar el comportamiento de su agente en la resoluci贸n del ambiente y comente sobre sus resultados.\n",
    "\n",
    "Adjunte el gif generado en su entrega (mejor a煤n si adem谩s adjuntan el gif en el markdown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "aItYF6sr6F_6",
    "outputId": "5b3ec38c-d067-4ff6-f78e-b3a995966a02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "IsLocked() == false",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-537aa530a9ad>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Entrenar el modelo (10,000 pasos de entrenamiento)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Lista para almacenar las recompensas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/td3/td3.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     ) -> SelfTD3:\n\u001b[0;32m--> 222\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     ) -> SelfOffPolicyAlgorithm:\n\u001b[0;32m--> 314\u001b[0;31m         total_timesteps, callback = self._setup_learn(\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36m_setup_learn\u001b[0;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorizedActionNoise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         return super()._setup_learn(\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36m_setup_learn\u001b[0;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_num_timesteps\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_obs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_episode_starts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0;31m# Retrieve unnormalized observation for saving into the buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mmaybe_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"options\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_seeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmaybe_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Seeds and options are only used once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected you to pass keyword argument {key} into reset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_reset_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mActType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mObsType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSupportsFloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \"\"\"\n\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    326\u001b[0m     ) -> tuple[WrapperObsType, dict[str, Any]]:\n\u001b[1;32m    327\u001b[0m         \u001b[0;34m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRenderFrame\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRenderFrame\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;34m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRenderFrame\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRenderFrame\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    326\u001b[0m     ) -> tuple[WrapperObsType, dict[str, Any]]:\n\u001b[1;32m    327\u001b[0m         \u001b[0;34m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRenderFrame\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRenderFrame\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_reset_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRenderFrame\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRenderFrame\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    328\u001b[0m     ):\n\u001b[1;32m    329\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_destroy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;31m# Bug's workaround for: https://github.com/Farama-Foundation/Gymnasium/issues/728\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36m_destroy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontactListener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_particles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDestroyBody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36m_clean_particles\u001b[0;34m(self, all_particle)\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_clean_particles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_particle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticles\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mall_particle\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mttl\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDestroyBody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: IsLocked() == false"
     ]
    }
   ],
   "source": [
    "# Crear el modelo con par谩metros personalizados\n",
    "model2 = TD3(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=0.001,   # Ajusta la tasa de aprendizaje\n",
    "    batch_size=16,         # Ajusta el tama帽o de los lotes\n",
    "    verbose=1              # Nivel de detalle en los logs\n",
    ")\n",
    "\n",
    "# Entrenar el modelo (10,000 pasos de entrenamiento)\n",
    "model2.learn(total_timesteps=10000)\n",
    "\n",
    "# Lista para almacenar las recompensas\n",
    "total_rewards = []\n",
    "\n",
    "# Repetir la simulaci贸n 5000 veces\n",
    "for _ in range(5000):\n",
    "    # Reiniciar el entorno\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Elegir una acci贸n usando el modelo entrenado\n",
    "        action, _states = model2.predict(observation)\n",
    "\n",
    "        # Tomar la acci贸n en el entorno\n",
    "        observation, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "        # Acumular la recompensa\n",
    "        total_reward += reward\n",
    "\n",
    "    # Guardar la recompensa total del episodio\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "# Convertir la lista de recompensas a un array de numpy para an谩lisis\n",
    "total_rewards = np.array(total_rewards)\n",
    "\n",
    "# Calcular el promedio y desviaci贸n est谩ndar\n",
    "average_reward = np.mean(total_rewards)\n",
    "std_deviation = np.std(total_rewards)\n",
    "\n",
    "# Reportar los resultados\n",
    "print(f\"Promedio de recompensas: {average_reward}\")\n",
    "print(f\"Desviaci贸n est谩ndar de las recompensas: {std_deviation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPUY-Ktgf2BO"
   },
   "source": [
    "## **2. Large Language Models (4.0 puntos)**\n",
    "\n",
    "En esta secci贸n se enfocar谩n en habilitar un Chatbot que nos permita responder preguntas 煤tiles a trav茅s de LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ4fPRRihGLe"
   },
   "source": [
    "### **2.0 Configuraci贸n Inicial**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/uqAs9atZH58AAAAd/config-config-issue.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Como siempre, cargamos todas nuestras API KEY al entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ud2Xm_k-hFJn",
    "outputId": "f08dcb8b-2dbe-46c7-8a13-e0454abcf553"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Google AI API key: 路路路路路路路路路路\n",
      "Enter your Tavily API key: 路路路路路路路路路路\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj9JvQUsgZZJ"
   },
   "source": [
    "### **2.1 Retrieval Augmented Generation (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://y.yarn.co/218aaa02-c47e-4ec9-b1c9-07792a06a88f_text.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsecci贸n es que habiliten un chatbot que pueda responder preguntas usando informaci贸n contenida en documentos PDF a trav茅s de **Retrieval Augmented Generation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrxOQroVnaZ5"
   },
   "source": [
    "#### **2.1.1 Reunir Documentos (0 puntos)**\n",
    "\n",
    "Reuna documentos PDF sobre los que hacer preguntas siguiendo las siguientes instrucciones:\n",
    "  - 2 documentos .pdf como m铆nimo.\n",
    "  - 50 p谩ginas de contenido como m铆nimo entre todos los documentos.\n",
    "  - Ideas para documentos: Documentos relacionados a temas acad茅micos, laborales o de ocio. Aprovechen este ejercicio para construir algo 煤til y/o relevante para ustedes!\n",
    "  - Deben ocupar documentos reales, no pueden utilizar los mismos de la clase.\n",
    "  - Deben registrar sus documentos en la siguiente [planilla](https://docs.google.com/spreadsheets/d/1Hy1w_dOiG2UCHJ8muyxhdKPZEPrrL7BNHm6E90imIIM/edit?usp=sharing). **NO PUEDEN USAR LOS MISMOS DOCUMENTOS QUE OTRO GRUPO**\n",
    "  - **Recuerden adjuntar los documentos en su entrega**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5D1tIRCi4oJJ",
    "outputId": "5cf17d37-14a4-47c1-f728-cde913aa389e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzq2TjWCnu15"
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "doc_paths = [\"prevencionycontrol.pdf\", \"ratasenpropiedad.pdf\",\"ratasyratones.pdf\"] \n",
    "\n",
    "assert len(doc_paths) >= 2, \"Deben adjuntar un m铆nimo de 2 documentos\"\n",
    "\n",
    "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in doc_paths)\n",
    "assert total_paginas >= 50, f\"P谩ginas insuficientes: {total_paginas}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r811-P71nizA"
   },
   "source": [
    "#### **2.1.2 Vectorizar Documentos (0.2 puntos)**\n",
    "\n",
    "Vectorice los documentos y almacene sus representaciones de manera acorde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2KZ_wK7xw8Zg",
    "outputId": "b58795bc-5e54-4cff-d200-07608408ade3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --quiet faiss-cpu langchain_community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xseuWa5oyEQu",
    "outputId": "9ee351e1-0b7f-4c24-fb62-2683d76d392a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_google_genai\n",
      "  Downloading langchain_google_genai-2.0.5-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (0.8.3)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (0.3.19)\n",
      "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (2.9.2)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.6.10)\n",
      "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.19.2)\n",
      "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.151.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.27.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.25.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.25.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain_google_genai) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain_google_genai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain_google_genai) (0.1.143)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain_google_genai) (24.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain_google_genai) (9.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.23.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.66.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.9)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.15->langchain_google_genai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain_google_genai) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain_google_genai) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain_google_genai) (1.0.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.1.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.68.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.62.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (3.2.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain_google_genai) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain_google_genai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain_google_genai) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain_google_genai) (3.10)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain_google_genai) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain_google_genai) (0.14.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.2.3)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain_google_genai) (1.2.2)\n",
      "Downloading langchain_google_genai-2.0.5-py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langchain_google_genai\n",
      "Successfully installed langchain_google_genai-2.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_google_genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-yXAdCSn4JM",
    "outputId": "801f385a-7199-4264-f16c-c3fbb7d4b2c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/google/colab/html/_background_server.py:103: DeprecationWarning: make_current is deprecated; start the event loop first\n",
      "  ioloop.make_current()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7dd83cdd30a0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Inicializar el cargador de documentos y cargar cada documento\n",
    "documents = []\n",
    "for path in doc_paths:\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents.extend(loader.load())  # Cargar documentos y a帽adirlos a la lista\n",
    "\n",
    "# Inicializar el divisor de texto\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(documents)  # Dividir documentos en chunks\n",
    "\n",
    "# Inicializar embeddings\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Crear vectorstore y vectorizar los documentos\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embedding)\n",
    "\n",
    "# Mostrar un resumen del vectorstore\n",
    "vectorstore\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAUkP5zrnyBK"
   },
   "source": [
    "#### **2.1.3 Habilitar RAG (0.3 puntos)**\n",
    "\n",
    "Habilite la soluci贸n RAG a trav茅s de una *chain* y gu谩rdela en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "t4-RURZwBGVm"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", # modelo de lenguaje\n",
    "    temperature=0, # probabilidad de \"respuestas creativas\"\n",
    "    max_tokens=None, # sin tope de tokens\n",
    "    timeout=None, # sin timeout\n",
    "    max_retries=2, # n煤mero m谩ximo de intentos\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "gPIySdDFn99l"
   },
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", # m茅todo de b煤squeda\n",
    "                                     search_kwargs={\"k\": 3}, # n掳 documentos a recuperar\n",
    "                                     )\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "retriever_chain = retriever | format_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "PymbEGKKAPo_"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "rag_template = '''\n",
    "Eres un asistente experto en el cuidado del hogar con respecto al control de roedores como ratas y ratones.\n",
    "Tu 煤nico rol es contestar preguntas del usuario a partir de informaci贸n relevante que te sea proporcionada.\n",
    "Responde siempre de la forma m谩s completa posible y usando toda la informaci贸n entregada.\n",
    "Responde s贸lo lo que te pregunten a partir de la informaci贸n relevante, NUNCA inventes una respuesta.\n",
    "\n",
    "Informaci贸n relevante: {context}\n",
    "Pregunta: {question}\n",
    "Respuesta 煤til:\n",
    "'''\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M2jBskJ9AgQ4",
    "outputId": "cf0a5427-95a5-42ef-b5ca-c2f500ad11f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La informaci贸n proporcionada no contiene datos sobre \"nulos y blancos\".  Por lo tanto, no puedo responder a tu pregunta.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever_chain, # context lo obtendremos del retriever_chain\n",
    "        \"question\": RunnablePassthrough(), # question pasar谩 directo hacia el prompt\n",
    "    }\n",
    "    | rag_prompt # prompt con las variables question y context\n",
    "    | llm # llm recibe el prompt y responde\n",
    "    | StrOutputParser() # recuperamos s贸lo la respuesta\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycg5S5i_n-kL"
   },
   "source": [
    "#### **2.1.4 Verificaci贸n de respuestas (0.5 puntos)**\n",
    "\n",
    "Genere un listado de 3 tuplas (\"pregunta\", \"respuesta correcta\") y analice la respuesta de su soluci贸n para cada una. 驴Su soluci贸n RAG entrega las respuestas que esperaba?\n",
    "\n",
    "Ejemplo de tupla:\n",
    "- Pregunta: 驴Qui茅n es el presidente de Chile?\n",
    "- Respuesta correcta: El presidente de Chile es Gabriel Boric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S_UiEn1hoZYR",
    "outputId": "664206f0-917b-4c8b-8640-4471121fd6df"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: 驴C贸mo elegir una empresa de control de plagas adecuada?\n",
      "Respuesta: Para elegir una empresa de control de plagas adecuada, debe seguir estos pasos:\n",
      "\n",
      "1. **Pregunte y entreviste a varias empresas:**  Obtenga informaci贸n sobre sus servicios y experiencia.\n",
      "\n",
      "2. **Pida referencias:** Consulte con vecinos y amigos para conocer sus experiencias con diferentes empresas.\n",
      "\n",
      "3. **Busque informaci贸n:** Revise directorios telef贸nicos e internet para encontrar empresas que ofrezcan \"control integral de plagas\".  Estas empresas suelen realizar revisiones, control, reparaciones y recomendaciones.\n",
      "\n",
      "\n",
      "\n",
      "Pregunta: 驴Qu茅 pasa si se detectan ratas en la calle?\n",
      "Respuesta: Si se detectan ratas en la calle, la ASPB (se asume que es una agencia de control de plagas) act煤a en la v铆a p煤blica, la red de alcantarillado y otros espacios p煤blicos como solares, equipamientos y mercados municipales.  Para reportarlo, se puede llamar al 010 o al 900 226 226, usar el Servicio de Atenci贸n en L铆nea, acudir a las Oficinas de Atenci贸n Ciudadana o usar la app Barcelona en el bolsillo.  Informar sobre la presencia de roedores es clave para mantener las plagas bajo control.\n",
      "\n",
      "\n",
      "\n",
      "Pregunta: 驴De que prefieren alimentarse las ratas?\n",
      "Respuesta: Las ratas prefieren alimentarse de aguacates, bayas, c铆tricos, semillas de pasto y para p谩jaros, hiedra, nueces, comida para mascotas, fruta madura y caracoles.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"驴C贸mo elegir una empresa de control de plagas adecuada?\",\n",
    "    \"驴Qu茅 pasa si se detectan ratas en la calle?\",\n",
    "    \"驴De que prefieren alimentarse las ratas?\"\n",
    "]\n",
    "\n",
    "# Iterar sobre las preguntas y obtener respuestas para cada una\n",
    "for query in questions:\n",
    "    response = rag_chain.invoke(query)  # Recupera la respuesta para cada pregunta\n",
    "    print(f\"Pregunta: {query}\")\n",
    "    print(f\"Respuesta: {response}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8d5zTMHoUgF"
   },
   "source": [
    "#### **2.1.5 Sensibilidad de Hiperpar谩metros (0.5 puntos)**\n",
    "\n",
    "Extienda el an谩lisis del punto 2.1.4 analizando c贸mo cambian las respuestas entregadas cambiando los siguientes hiperpar谩metros:\n",
    "- `Tama帽o del chunk`. (*驴C贸mo repercute que los chunks sean mas grandes o chicos?*)\n",
    "- `La cantidad de chunks recuperados`. (*驴Qu茅 pasa si se devuelven muchos/pocos chunks?*)\n",
    "- `El tipo de b煤squeda`. (*驴C贸mo afecta el tipo de b煤squeda a las respuestas de mi RAG?*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UDh_QgeXLGHc",
    "outputId": "900b0eff-539f-4d89-e567-19469907426f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta con chunk_size 100:\n",
      "Para controlar la poblaci贸n de ratas, primero debe buscar evidencia de su presencia.  Luego, controle la maleza, los matorrales y los arbustos, ya que las ratas suelen vivir en madrigueras.\n",
      "\n",
      "\n",
      "Respuesta con chunk_size 500:\n",
      "Para controlar la poblaci贸n de ratas, debe eliminar todo lo que necesitan para sobrevivir: comida, agua, refugio y rutas de movilidad.  Esto implica controlar la maleza, los matorrales y los arbustos, ya que las ratas suelen vivir en madrigueras debajo de ellos.  Se debe evitar el c茅sped alto, arbustos, matorrales y mantillo cerca de los cimientos de las construcciones.  Es necesario quitar la hiedra alrededor de las madrigueras, mantener un espacio descubierto de 6 pulgadas alrededor de las construcciones y podar debajo de los arbustos.  Finalmente, se debe conservar espacio entre las plantas y evitar la vegetaci贸n densa.\n",
      "\n",
      "\n",
      "Respuesta con chunk_size 1000:\n",
      "Para controlar la poblaci贸n de ratas, es fundamental un manejo adecuado del medio ambiente, enfoc谩ndose en eliminar sus tres necesidades b谩sicas: alimento, agua y refugio.  Esto implica pr谩cticas de higiene tanto en el hogar como en el entorno.  Espec铆ficamente, se recomienda cosechar y recoger diariamente frutas y nueces maduras; guardar la comida sobrante de las mascotas por la noche; mantener podadas las palmeras y plantas de yuca, y eliminar o podar la hiedra y otras plantas densas lejos de estructuras; reparar fugas de agua y eliminar el agua estancada.  Adicionalmente, se debe controlar la maleza, matorrales y arbustos, manteniendo un espacio descubierto de 6 pulgadas alrededor de las construcciones, podando debajo de los arbustos, conservando espacio entre las plantas y evitando la vegetaci贸n densa.  Se debe mantener los jardines libres de maleza y basura.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunk_sizes = [100, 500, 1000]  # Tama帽os de chunk a probar\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=50)\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=embedding)\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "    retriever_chain = retriever | format_docs\n",
    "    rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever_chain, # context lo obtendremos del retriever_chain\n",
    "        \"question\": RunnablePassthrough(), # question pasar谩 directo hacia el prompt\n",
    "    }\n",
    "    | rag_prompt # prompt con las variables question y context\n",
    "    | llm # llm recibe el prompt y responde\n",
    "    | StrOutputParser() # recuperamos s贸lo la respuesta\n",
    ")\n",
    "    # Ejecutar la b煤squeda y analizar las respuestas\n",
    "    query = \"驴C贸mo controlar la poblaci贸n de ratas?\"\n",
    "    response = rag_chain.invoke(query)\n",
    "    print(f\"Respuesta con chunk_size {chunk_size}:\\n{response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kTB8LpFs9hcU",
    "outputId": "66f5d0dd-9830-4fd3-b557-83f64a09bf7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta con k=1:\n",
      "Para controlar la poblaci贸n de ratas, es necesaria una prevenci贸n activa.  Esto se debe a que la presencia de ratas causa incomodidad, da帽os materiales y molestias.  Aunque en los 煤ltimos a帽os no hay evidencia de transmisi贸n de enfermedades de roedores a personas en la ciudad, las ratas pueden ser portadoras de enfermedades transmisibles, representando un riesgo para la salud.\n",
      "\n",
      "\n",
      "Respuesta con k=3:\n",
      "Para controlar la poblaci贸n de ratas, es fundamental un manejo adecuado del medio ambiente, enfoc谩ndose en eliminar sus tres necesidades b谩sicas: alimento, agua y refugio.  Esto implica pr谩cticas de higiene tanto en el hogar como en el entorno.  Espec铆ficamente, se recomienda cosechar y recoger diariamente frutas y nueces maduras; guardar la comida sobrante de las mascotas por la noche; mantener podadas las palmeras y plantas de yuca, y eliminar o podar la hiedra y otras plantas densas lejos de estructuras; reparar fugas de agua y eliminar el agua estancada.  Adicionalmente, se debe controlar la maleza, arbustos y c茅sped alto alrededor de las construcciones, manteniendo un espacio descubierto de al menos 6 pulgadas entre la vegetaci贸n y los cimientos.  Es importante conservar espacio entre las plantas y evitar la vegetaci贸n densa para reducir la disponibilidad de refugio.\n",
      "\n",
      "\n",
      "Respuesta con k=5:\n",
      "Para controlar la poblaci贸n de ratas, se debe seguir un plan de cinco pasos:\n",
      "\n",
      "**Paso uno: Buscar evidencia:**  Identifique los lugares donde viven las ratas buscando madrigueras (agujeros en la tierra o cemento de 1 a 4 pulgadas de ancho con bordes lisos, generalmente debajo de arbustos y plantas, con agujeros de entrada y salida), excrementos (generalmente cerca de la basura, h煤medo y oscuro indica presencia reciente), agujeros y roeduras en contenedores de basura, y senderos (marcas oscuras y grasosas en paredes y c茅sped).\n",
      "\n",
      "**Paso dos: Limpiar:**  Quite excremento y huellas. Desh谩gase del desorden. Controle la maleza, los matorrales y los arbustos.  Esto implica mantener un espacio descubierto de 6 pulgadas alrededor de las construcciones, podar debajo de los arbustos, conservar espacio entre las plantas, evitar vegetaci贸n densa, y mantener los jardines libres de maleza y basura.\n",
      "\n",
      "**Paso tres: Privarlas de comida:** Controle su basura. Mantenga la comida alejada.  Esto incluye cosechar y recoger diariamente frutas y nueces maduras, no dejar comida de mascotas afuera durante la noche, y mantener podadas las palmeras y plantas de yuca, eliminando o podando la hiedra de Argelia y otras plantas densas lejos de techos, paredes, bardas, postes y 谩rboles.\n",
      "\n",
      "**Paso cuatro: Cerrarles la puerta:** Selle las grietas y los agujeros peque帽os. Tape agujeros y aberturas grandes. Cierre las madrigueras.\n",
      "\n",
      "**Paso cinco: Exterminarlas:**  (La informaci贸n proporcionada no detalla m茅todos espec铆ficos de exterminio, solo menciona el cebo para roedores y la posibilidad de contratar una empresa de control de plagas).\n",
      "\n",
      "\n",
      "Respuesta con k=10:\n",
      "Para controlar la poblaci贸n de ratas, se debe seguir un proceso de cinco pasos:\n",
      "\n",
      "**Paso uno: Buscar evidencia.**  Identifique los lugares donde viven las ratas buscando madrigueras (agujeros en la tierra o cemento de 1 a 4 pulgadas de ancho con bordes lisos, generalmente con un agujero de entrada y salida), excremento (generalmente cerca de la basura, h煤medo y oscuro indica presencia de ratas), agujeros y roeduras en contenedores de basura, y senderos (marcas oscuras y grasosas a lo largo de paredes y c茅sped).\n",
      "\n",
      "**Paso dos: Limpiar.**  Elimine el excremento y las huellas lavando la zona con agua y una soluci贸n suave de cloro (1 parte de cloro por 10 partes de agua).  Desh谩gase del desorden (diarios, bolsas de papel, cartones, botellas, etc.) que proporciona refugio a las ratas. Limpie el s贸tano y el jard铆n, y guarde los art铆culos lejos de las paredes y elevados del suelo.  Es importante trabajar con los vecinos para una limpieza coordinada.\n",
      "\n",
      "**Paso tres: Privarlas de comida.**  Controle la basura, recoja la fruta y las nueces diariamente, no deje comida de mascotas afuera durante la noche, mantenga las palmeras y plantas de yuca podadas, y elimine o pode la hiedra y otras plantas densas lejos de techos, paredes, etc.\n",
      "\n",
      "**Paso cuatro: Cerrarles la puerta.** Selle las grietas y agujeros peque帽os, tape agujeros y aberturas grandes, y cierre las madrigueras.  Controle la maleza, los matorrales y los arbustos manteniendo un espacio descubierto de 6 pulgadas alrededor de las construcciones, podando debajo de los arbustos, conservando espacio entre las plantas y evitando vegetaci贸n densa. Mantenga los jardines libres de maleza y basura.\n",
      "\n",
      "**Paso cinco: Exterminarlas.** El cebo para roedores es una forma efectiva, pero su aplicaci贸n debe ser realizada por profesionales, excepto en casas unifamiliares sin inquilinos.  Si se contrata a una empresa de control de plagas, aseg煤rese de que sigan las pautas de seguridad y uso del cebo.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_values = [1, 3, 5, 10]  # Diferentes valores para k\n",
    "\n",
    "for k in k_values:\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "    retriever_chain = retriever | format_docs\n",
    "    rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever_chain, # context lo obtendremos del retriever_chain\n",
    "        \"question\": RunnablePassthrough(), # question pasar谩 directo hacia el prompt\n",
    "    }\n",
    "    | rag_prompt # prompt con las variables question y context\n",
    "    | llm # llm recibe el prompt y responde\n",
    "    | StrOutputParser() # recuperamos s贸lo la respuesta\n",
    ")\n",
    "    # Ejecutar la b煤squeda y analizar las respuestas\n",
    "    query = \"驴C贸mo controlar la poblaci贸n de ratas?\"\n",
    "    response = rag_chain.invoke(query)\n",
    "    print(f\"Respuesta con k={k}:\\n{response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wu_wngN5-DDy",
    "outputId": "c818eb32-ab18-4036-a549-10f6fb067f89"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta con search_type=similarity:\n",
      "Para controlar la poblaci贸n de ratas, es fundamental un manejo adecuado del medio ambiente, enfoc谩ndose en eliminar sus tres necesidades b谩sicas: alimento, agua y refugio.  Esto implica pr谩cticas de higiene tanto en el hogar como en el entorno.  Espec铆ficamente, se recomienda cosechar y recoger diariamente frutas y nueces maduras; guardar la comida sobrante de las mascotas por la noche; mantener podadas las palmeras y plantas de yuca, y eliminar o podar la hiedra y otras plantas densas lejos de estructuras; reparar fugas de agua y eliminar el agua estancada.  Adicionalmente, se debe controlar la maleza, matorrales y arbustos, manteniendo un espacio descubierto de 6 pulgadas alrededor de las construcciones, podando debajo de los arbustos, conservando espacio entre las plantas y evitando la vegetaci贸n densa.  Se debe mantener los jardines libres de maleza y basura.\n",
      "\n",
      "\n",
      "Respuesta con search_type=similarity_score_threshold:\n",
      "Para controlar la poblaci贸n de ratas, es fundamental un manejo adecuado del medio ambiente, enfoc谩ndose en eliminar sus tres necesidades b谩sicas: alimento, agua y refugio.  Esto implica pr谩cticas de higiene tanto en el hogar como en el entorno.  Espec铆ficamente, se recomienda cosechar y recoger diariamente frutas y nueces maduras; guardar la comida sobrante de las mascotas por la noche; mantener podadas las palmeras y plantas de yuca, y eliminar o podar la hiedra y otras plantas densas lejos de estructuras; reparar fugas de agua y eliminar el agua estancada.  Adicionalmente, se debe controlar la maleza, arbustos y c茅sped alto alrededor de las construcciones, manteniendo un espacio descubierto de al menos 6 pulgadas entre la vegetaci贸n y los cimientos.  Es importante conservar espacio entre las plantas y evitar la vegetaci贸n densa para reducir la disponibilidad de refugio.\n",
      "\n",
      "\n",
      "Respuesta con search_type=mmr:\n",
      "Para controlar la poblaci贸n de ratas, primero debe buscar evidencia de su presencia.  Esto incluye buscar madrigueras (agujeros en la tierra o cemento de 1 a 4 pulgadas de ancho con bordes lisos, generalmente con una entrada y salida), excremento (usualmente cerca de la basura, h煤medo y oscuro), agujeros y roeduras en contenedores de basura, y senderos oscuros y grasosos en paredes y c茅sped.\n",
      "\n",
      "Una vez localizada la evidencia, para controlarlas debe quitarles todo lo que necesitan para sobrevivir.  Para ello, almacene y coloque estaciones de cebo en lugares inaccesibles para ni帽os y mascotas.  Use 煤nicamente productos con N煤mero de Registro de la Agencia de Protecci贸n Ambiental (EPA) y evite productos ilegales como \"Tres Pasitos\". Reponga el cebo una vez consumido y deje las estaciones en su lugar por al menos dos semanas despu茅s de que cese toda actividad de las ratas.  Controle el 谩rea mensualmente.  Para mayor informaci贸n sobre pesticidas, contacte al Centro Nacional de Informaci贸n sobre Pesticidas al 1-800-858-7378.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_types = [\"similarity\", \"similarity_score_threshold\", \"mmr\"]  # Tipos de b煤squeda a probar\n",
    "\n",
    "for search_type in search_types:\n",
    "\n",
    "    if search_type == \"similarity_score_threshold\":\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=search_type,\n",
    "            search_kwargs={\"k\": 3, \"score_threshold\": 0.5}  # Ajusta score_threshold entre 0 y 1\n",
    "        )\n",
    "    else:\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=search_type,\n",
    "            search_kwargs={\"k\": 3}\n",
    "        )\n",
    "    retriever_chain = retriever | format_docs\n",
    "    rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever_chain, # context lo obtendremos del retriever_chain\n",
    "        \"question\": RunnablePassthrough(), # question pasar谩 directo hacia el prompt\n",
    "    }\n",
    "    | rag_prompt # prompt con las variables question y context\n",
    "    | llm # llm recibe el prompt y responde\n",
    "    | StrOutputParser() # recuperamos s贸lo la respuesta\n",
    ")\n",
    "\n",
    "    # Ejecutar la b煤squeda y analizar las respuestas\n",
    "    query = \"驴C贸mo controlar la poblaci贸n de ratas?\"\n",
    "    response = rag_chain.invoke(query)\n",
    "    print(f\"Respuesta con search_type={search_type}:\\n{response}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENJiPPM0giX8"
   },
   "source": [
    "### **2.2 Agentes (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/rcqnN2aJCSEAAAAd/secret-agent-man.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la secci贸n anterior, en esta secci贸n se busca habilitar **Agentes** para obtener informaci贸n a trav茅s de tools y as铆 responder la pregunta del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V47l7Mjfrk0N"
   },
   "source": [
    "#### **2.2.1 Tool de Tavily (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas al motor de b煤squeda **Tavily**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R6SLKwcWr0AG",
    "outputId": "a7859054-1c8f-4bde-a452-4ef07c83e861"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To answer this question effectively, I need to find reliable information on rat control methods.  A search engine like tavily_search_results_json will be helpful.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"effective rat control methods\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://effective-rat-control.cshelpjq.com/', 'content': 'Effective Rat Control  Nov 2024. effective rat control. most effective rat control, rat control methods, rat extermination methods, effective rat control methods, home rat extermination methods, best rat control methods, most effective rodent control, do it yourself rat extermination Sure, they able to success employees, ask yourself'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: The observation provides only one URL, and the content snippet is incomplete and doesn't offer specific methods. I need a more comprehensive search to get a better answer.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"how to control rat population\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://prepperswill.com/get-rid-of-rats/', 'content': 'Controlling the rat requires control of the total local rat population and not just individuals. Winter is the best time to start a control program because breeding levels are at their lowest, and it will take the rats 12 months to return to previous population levels. The next best times are spring and fall when it takes the rats six months to'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: The second search result offers more promising information, mentioning controlling the overall population and ideal times for intervention. However, it's still a brief snippet.  I need more detailed information on methods.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"rat control methods: sanitation, trapping, poisoning\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.domyown.com/how-to-get-rid-of-rats-a-525.html', 'content': 'How to Get Rid of Rats | DIY Rat Control Guide Rats Rats Rat Bait Stations Rat Poison/Bait Rat Traps Rats Rats Products needed : selected trap, bait for trapTrapping is the preferred method of indoor control. Trapping does not require the use of rat poisons indoors. Once the rat goes into the trap, they will stand on a pedal to eat the bait that has been placed inside. While you can use rat bait poison indoors, we strongly urge you not to do so. Controlling the rat population outdoors is usually done by using rat bait poison enclosed in tamper resistant rat bait stations. Load the rat bait blocks into the stations as directed on the product label.'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: This observation provides specific methods: trapping (preferred indoors), and poisoning (using bait stations outdoors).  This is a good starting point for a comprehensive answer.  I should also consider sanitation as a preventative measure.\n",
      "\n",
      "Thought: I now know the final answer.\n",
      "\n",
      "Final Answer:  Controlar la poblaci贸n de ratas requiere un enfoque multifac茅tico.  Los m茅todos efectivos incluyen:\n",
      "\n",
      "* **Saneamiento:** Eliminar fuentes de alimento y agua, sellar grietas y agujeros que permitan el acceso a edificios, mantener 谩reas limpias y libres de basura.  Esto es crucial para prevenir infestaciones.\n",
      "\n",
      "* **Trampeo:**  Es el m茅todo preferido para el control de ratas en interiores. Se utilizan trampas con cebos atractivos para capturarlas.\n",
      "\n",
      "* **Envenenamiento:**  Se utiliza principalmente en exteriores, con estaciones de cebo a prueba de manipulaciones que contienen veneno para ratas.  Es importante seguir las instrucciones del producto cuidadosamente y mantenerlo fuera del alcance de ni帽os y mascotas.\n",
      "\n",
      "Es importante recordar que el control de plagas puede requerir la ayuda de profesionales, especialmente en casos de infestaciones severas.  El mejor momento para iniciar un programa de control es en invierno, cuando la reproducci贸n es menor.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Controlar la poblaci贸n de ratas requiere un enfoque multifac茅tico.  Los m茅todos efectivos incluyen:\n",
      "\n",
      "* **Saneamiento:** Eliminar fuentes de alimento y agua, sellar grietas y agujeros que permitan el acceso a edificios, mantener 谩reas limpias y libres de basura.  Esto es crucial para prevenir infestaciones.\n",
      "\n",
      "* **Trampeo:**  Es el m茅todo preferido para el control de ratas en interiores. Se utilizan trampas con cebos atractivos para capturarlas.\n",
      "\n",
      "* **Envenenamiento:**  Se utiliza principalmente en exteriores, con estaciones de cebo a prueba de manipulaciones que contienen veneno para ratas.  Es importante seguir las instrucciones del producto cuidadosamente y mantenerlo fuera del alcance de ni帽os y mascotas.\n",
      "\n",
      "Es importante recordar que el control de plagas puede requerir la ayuda de profesionales, especialmente en casos de infestaciones severas.  El mejor momento para iniciar un programa de control es en invierno, cuando la reproducci贸n es menor.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain import hub\n",
    "\n",
    "react_prompt = hub.pull(\"hwchase17/react\") # template de ReAct\n",
    "\n",
    "search = TavilySearchResults(max_results = 1) # inicializamos tool\n",
    "tools = [search]\n",
    "\n",
    "agent = create_react_agent(llm, tools, react_prompt) # primero inicializamos el agente ReAct\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) # lo transformamos a AgentExecutor para habilitar la ejecuci贸n de tools\n",
    "response = agent_executor.invoke({\"input\": \"驴C贸mo controlar la poblaci贸n de ratas?\"})\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SonB1A-9rtRq"
   },
   "source": [
    "#### **2.2.2 Tool de Wikipedia (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas a **Wikipedia**.\n",
    "\n",
    "*Hint: Le puede ser de ayuda el siguiente [link](https://python.langchain.com/v0.1/docs/modules/tools/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bg9mDMA7FHbA",
    "outputId": "f67ce970-447c-45e8-c177-8fe595c28582"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=6ae70c0789b781ceaf064d304e1dd6e7529ef8b5be4719a1e22ad7d03e58e3e7\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "ehJJpoqsr26-",
    "outputId": "260c864c-fa66-4cdb-8f21-a3e844d3bb14"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-d3dd6515f135>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Inicializar el agente ReAct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_react_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreact_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Convertir el agente a un AgentExecutor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/react/agent.py\u001b[0m in \u001b[0;36mcreate_react_agent\u001b[0;34m(llm, tools, prompt, output_parser, tools_renderer, stop_sequence)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     prompt = prompt.partial(\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtools_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mtool_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/tools/render.py\u001b[0m in \u001b[0;36mrender_text_description\u001b[0;34m(tools)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{tool.name}{sig} - {tool.description}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{tool.name} - {tool.description}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mdescriptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=100)\n",
    "tool = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "\n",
    "# Inicializar el agente ReAct\n",
    "agent = create_react_agent(llm, tool, react_prompt)\n",
    "\n",
    "# Convertir el agente a un AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=[tool], verbose=True)\n",
    "\n",
    "# Ejecutar la consulta\n",
    "response = agent_executor.run({\"input\": \"驴C贸mo controlar la poblaci贸n de ratas?\"})\n",
    "\n",
    "# Imprimir la respuesta\n",
    "print(response[\"output\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvUIMdX6r0ne"
   },
   "source": [
    "#### **2.2.3 Crear Agente (0.3 puntos)**\n",
    "\n",
    "Crear un agente que pueda responder preguntas preguntas usando las *tools* antes generadas. Aseg煤rese que su agente responda en espa帽ol. Por 煤ltimo, guarde el agente en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pD1_n0wrsDI5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKV0JxK3r-XG"
   },
   "source": [
    "#### **2.2.4 Verificaci贸n de respuestas (0.3 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente y aseg煤rese que el agente est茅 ocupando correctamente las tools disponibles. 驴En qu茅 casos el agente deber铆a ocupar la tool de Tavily? 驴En qu茅 casos deber铆a ocupar la tool de Wikipedia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pqo2dsxvywW_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZbDTYiogquv"
   },
   "source": [
    "### **2.3 Multi Agente (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/r7QMJLxU4BoAAAAd/this-is-getting-out-of-hand-star-wars.gif\"\n",
    "\" width=\"450\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsecci贸n es encapsular las funcionalidades creadas en una soluci贸n multiagente con un **supervisor**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-iUfH0WvI6m"
   },
   "source": [
    "#### **2.3.1 Generando Tools (0.5 puntos)**\n",
    "\n",
    "Transforme la soluci贸n RAG de la secci贸n 2.1 y el agente de la secci贸n 2.2 a *tools* (una tool por cada uno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pw1cfTtvv1AZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQYNjT_0vPCg"
   },
   "source": [
    "#### **2.3.2 Agente Supervisor (0.5 puntos)**\n",
    "\n",
    "Habilite un agente que tenga acceso a las tools del punto anterior y pueda responder preguntas relacionadas. Almacene este agente en una variable llamada supervisor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yv2ZY0BAv1RD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea3zWlvyvY7K"
   },
   "source": [
    "#### **2.3.3 Verificaci贸n de respuestas (0.25 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente repitiendo las preguntas realizadas en las secciones 2.1.4 y 2.2.4 y comente sus resultados. 驴C贸mo var铆an las respuestas bajo este enfoque?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_1t0zkgv1qW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qb8bdAmYvgwn"
   },
   "source": [
    "#### **2.3.4 An谩lisis (0.25 puntos)**\n",
    "\n",
    "驴Qu茅 diferencias tiene este enfoque con la soluci贸n *Router* vista en clases? Nombre al menos una ventaja y desventaja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAUlJxqoLK5r"
   },
   "source": [
    "`escriba su respuesta ac谩`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JWVSuWiZ8Mj"
   },
   "source": [
    "### **2.4 Memoria (Bonus +0.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/Gs95aiElrscAAAAd/memory-unlocked-ratatouille-critic.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una de las principales falencias de las soluciones que hemos visto hasta ahora es que nuestro chat no responde las interacciones anteriores, por ejemplo:\n",
    "\n",
    "- Pregunta 1: \"Hola! mi nombre es Sebasti谩n\"\n",
    "  - Respuesta esperada: \"Hola Sebasti谩n! ...\"\n",
    "- Pregunta 2: \"Cual es mi nombre?\"\n",
    "  - Respuesta actual: \"Lo siento pero no conozco tu nombre :(\"\n",
    "  - **Respuesta esperada: \"Tu nombre es Sebasti谩n\"**\n",
    "\n",
    "Para solucionar esto, se les solicita agregar un componente de **memoria** a la soluci贸n entregada en el punto 2.3.\n",
    "\n",
    "**Nota: El Bonus es v谩lido <u>s贸lo para la secci贸n 2 de Large Language Models.</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6Y7tIPJLPfB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFc3jBT5g0kT"
   },
   "source": [
    "### **2.5 Despliegue (0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/IytHqOp52EsAAAAd/you-get-a-deploy-deploy.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una vez tengan los puntos anteriores finalizados, toca la etapa de dar a conocer lo que hicimos! Para eso, vamos a desplegar nuestro modelo a trav茅s de `gradio`, una librer铆a especializada en el levantamiento r谩pido de demos basadas en ML.\n",
    "\n",
    "Primero instalamos la librer铆a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8TsvnCPbkIA"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJBztEUovKsF"
   },
   "source": [
    "Luego s贸lo deben ejecutar el siguiente c贸digo e interactuar con la interfaz a trav茅s del notebook o del link generado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3KedQSvg1-n"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def agent_response(message, history):\n",
    "  '''\n",
    "  Funci贸n para gradio, recibe mensaje e historial, devuelte la respuesta del chatbot.\n",
    "  '''\n",
    "  # get chatbot response\n",
    "  response = ... # rellenar con la respuesta de su chat\n",
    "\n",
    "  # assert\n",
    "  assert type(response) == str, \"output de route_question debe ser string\"\n",
    "\n",
    "  # \"streaming\" response\n",
    "  for i in range(len(response)):\n",
    "    time.sleep(0.015)\n",
    "    yield response[: i+1]\n",
    "\n",
    "gr.ChatInterface(\n",
    "    agent_response,\n",
    "    type=\"messages\",\n",
    "    title=\"Chatbot MDS7202\", # Pueden cambiar esto si lo desean\n",
    "    description=\"Hola! Soy un chatbot muy 煤til :)\", # tambi茅n la descripci贸n\n",
    "    theme=\"soft\",\n",
    "    ).launch(\n",
    "        share=True, # pueden compartir el link a sus amig@s para que interactuen con su chat!\n",
    "        debug = False,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
